{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "eda-twitter-sentiment-analysis-using-nn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gr-30/DL/blob/main/eda-twitter-sentiment-analysis-using-nn_emb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxsI9lWTKCiQ"
      },
      "source": [
        "# <img src=\"https://miro.medium.com/max/700/1*0OVev9mGkNJblfkOxkknAQ.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPJcfIlrg8Nl",
        "outputId": "51107cd6-e74b-4002-8c1c-243b4f3e3bc1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020638,
          "end_time": "2020-12-02T21:20:36.560772",
          "exception": false,
          "start_time": "2020-12-02T21:20:36.540134",
          "status": "completed"
        },
        "tags": [],
        "id": "XAgNSrR5KCiS"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">  \n",
        "<h1><strong>Introduction</strong></h1>\n",
        "    <p>Millions of people are using Twitter and expressing their emotions like happiness, sadness, angry, etc. The Sentiment analysis is also about detecting the emotions, opinion, assessment, attitudes, and took this into consideration as a way humans think. Sentiment analysis classifies the emotions into classes such as positive or negative. Nowadays, industries are interested to use¬†textual data for semantic analysis to extract the view of people about their products and services. Sentiment analysis is very important for them to know the customer satisfaction level and they can improve their services accordingly. To work on the text data, they try to extract the data from social media platforms. There are a lot of social media sites like Google Plus, Facebook, and Twitter that allow expressing opinions, views, and emotions about certain topics and events. Microblogging site Twitter is expanding rapidly among all other online social media networking sites with about 200 million users. Twitter was founded in 2006 and currently, it is the most famous microblogging platform. In 2017 2 million users shared 8.3 million tweets in one hour. Twitter users use to post their thoughts, emotions, and messages on their profiles, called tweets. Words limit of a single tweet has 140 characters. Twitter sentiment analysis based on the NLP (natural language processing) field. For tweets text, we use NLP techniques like tokenizing the words, removing the stop words like I, me, my, our, your, is, was, etc. Natural language processing also plays a part to preprocess the data like cleaning the text and removing the special characters and punctuation marks. Sentimental analysis is very important because we can know the trends of people‚Äôs emotions on specific topics with their tweets.</p>\n",
        "    <br>\n",
        "        <hr>\n",
        "      <b>Problem description/definition: </b> \n",
        "    <hr>\n",
        "<ul>\n",
        "    <li>To devise a sentimental analyzer for overcoming the challenges to identify the twitter tweets text sentiments (positive, negative) by implementing neural network using tensorflow</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "<hr>\n",
        "<b>Evolution measures: </b> \n",
        "<hr>\n",
        "<ul>\n",
        "<p> After training the model, we apply the evaluation measures to check that how the model is getting predictions. We will use the following evaluation measures to evaluate the performance of the models:</p>\n",
        "    <li>Accuracy</li>\n",
        "    <li>Confusion matrix with plot</li>\n",
        "    <li>ROC Curve</li>\n",
        "</ul>\n",
        "<hr>\n",
        "<b>Technical Approach</b>\n",
        "<hr>\n",
        "<p>We are using python language in the implementations and Jupter Notebook that support the machine learning and data science projects. We will build tensorflow based model. We will use Sentiment 140 dataset and split that data into 70% for training and 30% for the testing purposes. After training on the model, we will evaluate the model to evaluate the performance of trained model</p>\n",
        " \n",
        "<hr>\n",
        "<b>Source of Data: </b> \n",
        "<hr> \n",
        " <a href=\"https://www.kaggle.com/kazanova/sentiment140\">https://www.kaggle.com/kazanova/sentiment140</a>\n",
        "   \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.065466,
          "end_time": "2020-11-30T07:38:51.578836",
          "exception": false,
          "start_time": "2020-11-30T07:38:51.513370",
          "status": "completed"
        },
        "tags": [],
        "id": "NpAnrT8OKCiU"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<h2><center><strong>Importing Python Libraries üìï üìó üìò üìô</strong></center></h2>\n",
        "        \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oykILU30KCiU"
      },
      "source": [
        "- Libraries are important and we call them to perform the different actions on our data and for training the models.\n",
        "- Its a first step to load the library to perform the specific task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E4TvIVLMa73",
        "scrolled": true,
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e59e83-ec4a-43d6-afc6-d594f0bd751f"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3v6JNFSKzfw"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Mtech DSE BITS/Sem3/DL/Assignment1/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u0uyx61KCiW"
      },
      "source": [
        "<h4> We are uisng the following versions of the libraries:</h4>\n",
        "\n",
        "- numpy == 1.18.5 \n",
        "\n",
        "- pandas == 1.1.3\n",
        "\n",
        "- tensorflow ==1.7.0\n",
        "\n",
        "- keras == 2.4.3\n",
        "\n",
        "- nltk ==3.5\n",
        "\n",
        "- seaborn ==0.11.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cR1Q7gLKCiX"
      },
      "source": [
        "<h4>How we can install the libraries in python?</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mszLQeqVKCiX"
      },
      "source": [
        "<h4>To install the python library is very easy</h4>\n",
        "- pip install name_of_library \n",
        "<h5> Like if you wanted to install tensorflow? </h5>\n",
        "- pip install tensforflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.065466,
          "end_time": "2020-11-30T07:38:51.578836",
          "exception": false,
          "start_time": "2020-11-30T07:38:51.513370",
          "status": "completed"
        },
        "tags": [],
        "id": "7FHGqd9NKCiX"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<h2><center><strong>Loading the data üìÅ üìÇ</strong></center></h2>\n",
        "        \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "EgjoX1kWKCiY"
      },
      "source": [
        "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.065466,
          "end_time": "2020-11-30T07:38:51.578836",
          "exception": false,
          "start_time": "2020-11-30T07:38:51.513370",
          "status": "completed"
        },
        "tags": [],
        "id": "ewWFou7rKCiY"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<h2><center><strong>Exploratory data analysis üîé üìä</strong></center></h2>\n",
        "        \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOYepGcYKCiZ"
      },
      "source": [
        "#### Five top records of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "o27SeogeKCiZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "91f8e866-6a93-4c8c-dbcc-d2ef9f49b64f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>time</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811372</td>\n",
              "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>joy_wolf</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                               text\n",
              "0      0  ...  is upset that he can't update his Facebook by ...\n",
              "1      0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "2      0  ...    my whole body feels itchy and like its on fire \n",
              "3      0  ...  @nationwideclass no, it's not behaving at all....\n",
              "4      0  ...                      @Kwesidei not the whole crew \n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTEv12ZUKCia"
      },
      "source": [
        "#### Five last records of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Cj8PV1c-KCia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "970ab647-4e1d-43ac-bbf5-3c40621832ad"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>time</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599994</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label  ...                                               text\n",
              "1599994      4  ...  Just woke up. Having no school is the best fee...\n",
              "1599995      4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599996      4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599997      4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599998      4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgEQYqKKCia"
      },
      "source": [
        "#### Coloumns/features in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "GsDEwSDGKCia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6e9724-e57d-4c38-c16f-b7b463a8e95d"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'time', 'date', 'query', 'username', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsgnRLrKCib"
      },
      "source": [
        "#### Length of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "KGFjX1_KKCib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512d163d-dea7-4a6b-ea20-07af83a3d04b"
      },
      "source": [
        "print('lenght of data is', len(data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lenght of data is 1599999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ83WJfUKCib"
      },
      "source": [
        "#### Shape of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "7BloB18eKCib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd629b7-9a52-4047-d5e9-430c0b2df1a4"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1599999, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUJyZyLFKCic"
      },
      "source": [
        "#### Data information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "trusted": false,
        "id": "RzY8eV-YKCic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515f221a-8178-44d4-fd7f-5bf7c8771ba9"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599999 entries, 0 to 1599998\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count    Dtype \n",
            "---  ------    --------------    ----- \n",
            " 0   label     1599999 non-null  int64 \n",
            " 1   time      1599999 non-null  int64 \n",
            " 2   date      1599999 non-null  object\n",
            " 3   query     1599999 non-null  object\n",
            " 4   username  1599999 non-null  object\n",
            " 5   text      1599999 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDLzeBmoKCic"
      },
      "source": [
        "#### Data types of all coloumns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "qOGeo1NwKCic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6d6658-f62c-435a-ca0c-0d557d560863"
      },
      "source": [
        "data.dtypes"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label        int64\n",
              "time         int64\n",
              "date        object\n",
              "query       object\n",
              "username    object\n",
              "text        object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn3MeiXbKCic"
      },
      "source": [
        "#### Checking Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "2sis1zAMKCid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90000796-b6b8-4e30-87d4-e31064c02bf1"
      },
      "source": [
        "np.sum(data.isnull().any(axis=1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfQdCQMVKCid"
      },
      "source": [
        "#### Rows and columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "c_t0JWayKCid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c563c2-d912-486c-e41b-c1ae63d2ac05"
      },
      "source": [
        "print('Count of columns in the data is:  ', len(data.columns))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of columns in the data is:   6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "qCh06swEKCid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a50ea3-0beb-4b0a-ad38-a4dad0ea56e1"
      },
      "source": [
        "print('Count of rows in the data is:  ', len(data))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of rows in the data is:   1599999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.065466,
          "end_time": "2020-11-30T07:38:51.578836",
          "exception": false,
          "start_time": "2020-11-30T07:38:51.513370",
          "status": "completed"
        },
        "tags": [],
        "id": "D0vIudaNKCid"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<h1><center><strong>Data Preparation üìù</strong></center></h1>\n",
        "\n",
        " <ul style=\"list-style-type:circle;\">\n",
        "     <h6>\n",
        "         <li>Selection of interested coloumns</li>\n",
        "         <br>\n",
        "          <li>Assinged 1 to class 4</li>\n",
        "         <br>\n",
        "          <li>Took one fourth data so we can run on our machine easily</li>\n",
        "         <br>\n",
        "          <li>Combined positive and negative tweets</li>\n",
        "         <br>\n",
        "    <li>We will convert the text in lower case for the further working on tweet text. </li>\n",
        "         <br>\n",
        "         <li>We will clean and remove the stop words(of, a, in etc) from statement because these words are not useuseful to support the labels of sentiments  data</li>\n",
        "<br>\n",
        "         <li>We will clean and remove the punctuations because these are the noise in the data and not meaningfull</li>\n",
        "         <br>\n",
        "         <li>We will clean and remove repeating characters in the words</li>\n",
        "         <br>\n",
        "         <li>We will clean and remove emails</li>\n",
        "         <br>\n",
        "         <li>We will clean and remove URL's</li>\n",
        "         <br>\n",
        "         <li>We will clean and remove the numbers in the data</li>\n",
        "         <br>\n",
        "         <li>We will apply tokenization(to separate the sentence into words)</li>\n",
        "         <br>\n",
        "         <li>We will apply stemming and lemmatization on the text. The concept of both is following:</li>\n",
        "         <img src=\"https://lh3.googleusercontent.com/3wumK8lGLhKpD2Fhbu35I7wWf6OSpF_erX9T7FX9WQCE5_HBKMJpKOZNximlzlTG5882QUWcL-_lFLJd0-RIo4uHDaO7cK8aEnw2Tm2-5xPwjYS3ls6fYefeGAVGb1WUGrXafJrC\">\n",
        "          <br>\n",
        "         <li>We will apply stemming on the tweet text.</li>\n",
        "         <br>\n",
        "         <li>We will apply lemmatization on the tweet text.</li>\n",
        "         <br>\n",
        "         <li>Separated input feature and labels</li>\n",
        "         <br>\n",
        "         <li>Extracted features from input feature</li>\n",
        "         <br>\n",
        "         <li>Separated the 70% data for training and 30% data for testing</li>\n",
        "</h6>\n",
        "</ul>\n",
        "   \n",
        "        \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7uYpcHKCie"
      },
      "source": [
        "#### Selecting the text and label coloumn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "7uH3hZt1KCie"
      },
      "source": [
        "data=data[['text','label']]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi8vSNbpKCie"
      },
      "source": [
        "#### Assigning 1 to Positive sentment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "6ONumEf8KCie"
      },
      "source": [
        "data['label'][data['label']==4]=1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHzTHBxQKCie"
      },
      "source": [
        "#### Separating positive and negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "VtGe92SpKCif"
      },
      "source": [
        "data_pos = data[data['label'] == 1]\n",
        "data_neg = data[data['label'] == 0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdmAvZWAKCif"
      },
      "source": [
        "#### taking one fourth data so we can run on our machine easily "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "PYZBcXZTKCif"
      },
      "source": [
        "data_pos = data_pos.iloc[:int(20000)]\n",
        "data_neg = data_neg.iloc[:int(20000)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TSzAsL_KCif"
      },
      "source": [
        "#### Combining positive and negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "PsAkK3z0KCif"
      },
      "source": [
        "data = pd.concat([data_pos, data_neg])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An7YunEFKCif"
      },
      "source": [
        "#### Making statement text in lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "_RaUAK1dKCif"
      },
      "source": [
        "data['text']=data['text'].str.lower()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "aPx0TSAZKCig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0049c5e-99b2-438d-bda1-898b760a7205"
      },
      "source": [
        "data['text'].tail()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                            one more day of holidays \n",
              "19996    feeling so down right now .. i hate you damn h...\n",
              "19997    geez,i hv to read the whole book of personalit...\n",
              "19998    i threw my sign at donnie and he bent over to ...\n",
              "19999    @heather2711 good thing i didn't find any then...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJLdAkW2KCig"
      },
      "source": [
        "#### Cleaning and removing Stop words of english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Vs-0ZoyxKCig"
      },
      "source": [
        "stopwords_list = stopwords.words('english')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "uuRag4CjKCig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d545d280-606b-4755-8a04-4a3398059c93"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hnKomuMKCig"
      },
      "source": [
        "#### Cleaning and removing the above stop words list from the tweet text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "GKSuPOutKCig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a84639-eefc-45bb-870a-23ef7c8b7ce2"
      },
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
        "data['text'].head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "799999                love @health4uandpets u guys r best!!\n",
              "800000    im meeting one besties tonight! cant wait!! - ...\n",
              "800001    @darealsunisakim thanks twitter add, sunisa! g...\n",
              "800002    sick really cheap hurts much eat real food plu...\n",
              "800003                      @lovesbrooklyn2 effect everyone\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQPlZmWHKCih"
      },
      "source": [
        "#### Cleaning and removing punctuations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "iNTNbsyfKCih"
      },
      "source": [
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "2OPjkNyDKCih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cffe9eb2-688c-4a87-ce9a-801e6b5b6527"
      },
      "source": [
        "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "data['text'].tail()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                     one day holidays\n",
              "19996                     feeling right  hate damn humprey\n",
              "19997    geezi hv read whole book personality types emb...\n",
              "19998     threw sign donnie bent get thingee made sad face\n",
              "19999    heather2711 good thing find none ones like com...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR4ZyrzMKCih"
      },
      "source": [
        "#### Cleaning and removing repeating characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "tES79mpnKCii"
      },
      "source": [
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "NA41bbxKKCii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb95eb6-90a5-462d-db5a-9df488714003"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
        "data['text'].tail()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                     one day holidays\n",
              "19996                       feling right hate damn humprey\n",
              "19997    gezi hv read whole bok personality types embar...\n",
              "19998       threw sign donie bent get thinge made sad face\n",
              "19999    heather271 god thing find none ones like come ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzsSFVHzKCii"
      },
      "source": [
        "#### Cleaning and removing email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "NlJLsVTJKCii"
      },
      "source": [
        "def cleaning_email(data):\n",
        "    return re.sub('@[^\\s]+', ' ', data)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "A3P78Z-9KCii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd96f471-5bf9-4e29-da5b-d653e2607661"
      },
      "source": [
        "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
        "data['text'].tail()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                     one day holidays\n",
              "19996                       feling right hate damn humprey\n",
              "19997    gezi hv read whole bok personality types embar...\n",
              "19998       threw sign donie bent get thinge made sad face\n",
              "19999    heather271 god thing find none ones like come ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuMtjDfUKCij"
      },
      "source": [
        "#### Cleaning and removing URL's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Sau6uy7yKCij"
      },
      "source": [
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "FHokNJf8KCij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c977a38-e252-4009-fc9e-eb1b57199535"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
        "data['text'].tail()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                     one day holidays\n",
              "19996                       feling right hate damn humprey\n",
              "19997    gezi hv read whole bok personality types embar...\n",
              "19998       threw sign donie bent get thinge made sad face\n",
              "19999    heather271 god thing find none ones like come ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fogSC664KCij"
      },
      "source": [
        "#### Cleaning and removing Numeric numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "o97XCJ3iKCij"
      },
      "source": [
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "de04gA7CKCik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b2a7ea-4b38-4e85-e3c6-8074cdfa8c09"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
        "data['text'].tail()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                     one day holidays\n",
              "19996                       feling right hate damn humprey\n",
              "19997    gezi hv read whole bok personality types embar...\n",
              "19998       threw sign donie bent get thinge made sad face\n",
              "19999    heather god thing find none ones like come siz...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8yagt4gKCik"
      },
      "source": [
        "#### Getting tokenization of tweet text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "tEHWNXC6KCik"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "data['text'] = data['text'].apply(tokenizer.tokenize)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "FeoLk5PNKCik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c60a26-bd62-44a5-9860-6f94a1cef41e"
      },
      "source": [
        "data['text'].tail()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                 [one, day, holidays]\n",
              "19996                 [feling, right, hate, damn, humprey]\n",
              "19997    [gezi, hv, read, whole, bok, personality, type...\n",
              "19998    [threw, sign, donie, bent, get, thinge, made, ...\n",
              "19999    [heather, god, thing, find, none, ones, like, ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVJTr7UIKCik"
      },
      "source": [
        "#### Applying Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "uM3o7_cKKCik"
      },
      "source": [
        "st = nltk.PorterStemmer()\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return data\n",
        "\n",
        "data['text']= data['text'].apply(lambda x: stemming_on_text(x))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "YxNFHsklKCil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ec07e2-1c6a-41dc-85b7-790c246748b1"
      },
      "source": [
        "data['text'].tail()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                 [one, day, holidays]\n",
              "19996                 [feling, right, hate, damn, humprey]\n",
              "19997    [gezi, hv, read, whole, bok, personality, type...\n",
              "19998    [threw, sign, donie, bent, get, thinge, made, ...\n",
              "19999    [heather, god, thing, find, none, ones, like, ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwbjRMifKCil"
      },
      "source": [
        "#### Applying Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "-IjbC6DXKCil"
      },
      "source": [
        "lm = nltk.WordNetLemmatizer()\n",
        "def lemmatizer_on_text(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return data\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "DsWURUZOKCil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203a4a89-2d2e-44be-a302-da66443550b3"
      },
      "source": [
        "data['text'].tail()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19995                                 [one, day, holidays]\n",
              "19996                 [feling, right, hate, damn, humprey]\n",
              "19997    [gezi, hv, read, whole, bok, personality, type...\n",
              "19998    [threw, sign, donie, bent, get, thinge, made, ...\n",
              "19999    [heather, god, thing, find, none, ones, like, ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KZ8QSAA0asZ"
      },
      "source": [
        "joinedData = [' '.join(d) for d in data.text.values]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "631IIJxf1cg5",
        "outputId": "74136061-2eca-4787-862c-2c83205a7ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "joinedData"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love healthuandpets u guys r best',\n",
              " 'im meting one besties tonight cant wait girl talk',\n",
              " 'darealsunisakim thanks twiter ad sunisa got met hin show dc area swetheart',\n",
              " 'sick realy cheap hurts much eat real fod plus friends make soup',\n",
              " 'lovesbroklyn efect everyone',\n",
              " 'productofear tel burst laughing realy loud thanks making come sulk',\n",
              " 'rkeithil thans response ihad already find answer',\n",
              " 'kepinupwkris jealous hope great time vegas like acms love show',\n",
              " 'tomcfly ah congrats mr fletcher finaly joining twiter',\n",
              " 'evoip responded stupid cat helping type forgive erors',\n",
              " 'crazy day schol hours straight watch hils spencerprat told to ha hapy birthday jb',\n",
              " 'naughtyhaughty forget two half men love show',\n",
              " 'nileyjileyluver haha wory get hang it',\n",
              " 'soundwav least one feling lost may cause many later usual nights already adicting',\n",
              " 'lutheranluciol make sure dm post link video ltlolgtso mis beter get permision blesing first',\n",
              " 'aded twetie new iphone',\n",
              " 'michelardi realy know think globe yeah sana gumaling na ko para alam ko na din kung makakasama ako',\n",
              " 'nicolerichie picture swet',\n",
              " 'catching emails rs random bacn im cuting early tonight pm diner lauraw',\n",
              " 'dancing around rom pjs jaming ipod geting dizy wel twiter asked',\n",
              " 'place peps contest thanks voting anyways',\n",
              " 'going bed godnight everyone swet dreams htptwitpicomye',\n",
              " 'litlelumen walking put deposit tomorow',\n",
              " 'folowinq dachesterfrench shud tha',\n",
              " 'lordpov meant ad back quotwitering toilet cubicle somewherequot',\n",
              " 'aw im holding new pupy wel hes mine hes cutie',\n",
              " 'ijohn kiteh slepin crotch proves likes',\n",
              " 'dramabeats agred',\n",
              " 'reaching amritsar hour if find bus wagah border pm htpbkitecomfuj',\n",
              " 'albinla thinking tonight let first interview famous super star',\n",
              " 'hapy spend time family',\n",
              " 'finaly going bed tired gona watch hils didnt',\n",
              " 'efing tired throat hurting oh got crazy craving pina coladabanana slushie',\n",
              " 'deon upload di indowebster dong bang',\n",
              " 'wisdomous welcome glad enjoyed it',\n",
              " 'hawaishelz hahaha omg wer laughin hok cuz das rolryt sheldawg',\n",
              " 'sickwiththepen aw pokie fel beter il pray youand bag nurse',\n",
              " 'yay found new cudle budy',\n",
              " 'think met first quotsnobquot twiter tonight bad life goes on',\n",
              " 'acros universe slep rehearsal tomorow',\n",
              " 'anabele dry swet potato huh',\n",
              " 'jonathanrknight hi jon great hear you se cruise canot wait hope wel knight bus loved',\n",
              " 'long conversation mom phone',\n",
              " 'suitelifeofkel yay lol requested her say',\n",
              " 'practicing linesmanerismsvoices upcoming feature shot probably driving brock crazy',\n",
              " 'going go read new mon rd time cant get enough twilight series',\n",
              " 'today two month aniversary love so much diana omg dont even know tar hels nca wot',\n",
              " 'bored',\n",
              " 'stevecla walpaper red square asking',\n",
              " 'rkeithil thanks response ihad already find answer',\n",
              " 'popemelo wel yeah noticed cuz im b to',\n",
              " 'tryna get inspired thats',\n",
              " 'kourtneykardash cant get early enough night workouts much beter',\n",
              " 'ladygaga cant wait se ur hot as austin wot wot and love bob purple went royal color way wel',\n",
              " 'quotablebufy got bunch bufy songs to one faves quotvivianquot nerf herder faith met spike bufys body',\n",
              " 'devunity walpaper check gt htptwitpicomye',\n",
              " 'morning twetland long day ahead hope everyone great day',\n",
              " 'uploading pictures friendster',\n",
              " 'ashct aw dont love love days thanks pic p',\n",
              " 'nodlebox amanda tonight',\n",
              " 'unavion sory im like diferent things kevin jonas girlfriend',\n",
              " 'fuzeb serious singing like whoa hehe lsd maybe jk lolol',\n",
              " 'loving life loving',\n",
              " 'apothecaryjeri love sicilians best damn piza planet says i',\n",
              " 'loves chocolate milk gf yeah',\n",
              " 'going suter crek tomorow tour old mine',\n",
              " 'bodycoach il lok cho personaly like strawbery im going become coach son',\n",
              " 'litlefish gues live side earth east mesa losiento',\n",
              " 'paulapaige apart sore tryouts',\n",
              " 'failed physics test homo ken holidays days go',\n",
              " 'observed update lok alexa website htptinyurlcomcgzf loks great sems alexa improving',\n",
              " 'leiabox tel us it im totaly geking right',\n",
              " 'theragingocean bonjour spacecowboyi wish either i work kids go whenever want go wherever want',\n",
              " 'rebeca godnight',\n",
              " 'brother sick lol quite weird',\n",
              " 'drdrew gives hug cokie hope fel beter',\n",
              " 'jonathanrknight hey jon real hope take care yourselfwe want get rundown hugs',\n",
              " 'wrote toilet wal corect por speling highlighter wash of hardcore',\n",
              " 'god tech meting clubzone diner sushi',\n",
              " 'matgaloway thanks hok carlyrush sugesting bro rock',\n",
              " 'jefswarens depends version thought one know go like',\n",
              " 'speakashley hand stil prety weak cant like punch anyone yet turn knobs doesnt hurt much',\n",
              " 'honey lurkers interesting',\n",
              " 'saravananp mine b north stil ned decide aru hithavaru ninage e movarolage vote',\n",
              " 'trstfndby um bought shit koreans oach highest quality baby paid extra get real tag sewed',\n",
              " 'eating icecream chocolate toping',\n",
              " 'iamdidy teling me finished crunches step didy lets go',\n",
              " 'itsyvone dang know posible talk long whatd guys talk',\n",
              " 'lauruy west mostly san jose san francisco',\n",
              " 'niki working as bike trying recover kne injury god her',\n",
              " 'marymayblod would love come help unpack let know ned sat afternon',\n",
              " 'shenagians team awesomelt',\n",
              " 'listening music chilingil probably regret geting work donebut til im gona kick back',\n",
              " 'study think amp grow rich wow amazing timeles law atraction is were step program',\n",
              " 'bitenbyboks oh god im glad feling beter ive realy god itl crazy wek though god way',\n",
              " 'days til chris comes home days til sumer',\n",
              " 'davidgideon yeah days no around ish',\n",
              " 'therealjspace first one tel twet drinking lol',\n",
              " 'itsane kay thanks',\n",
              " 'godnight',\n",
              " 'determined get back basebal years since ive realy watched sport anyone interested going game',\n",
              " 'beycah god beca god slep',\n",
              " 'misnapyboy ur home where oh nkow lexington ky rite next',\n",
              " 'sen preview movie quotobsesedquot w beyonce wow one crazy broad movie amp beyonce sems go nuts to',\n",
              " 'easy rider til fal aslep',\n",
              " 'miketomalaris im very very excited live broadcast parisroubaix',\n",
              " 'real god moment mis much',\n",
              " 'beliestobabies two to know people survive winter without jungle gyms home',\n",
              " 'jaredleto started work last one holidayholiday',\n",
              " 'watchin grek knighty knight',\n",
              " 'working media rom design love love love client profile',\n",
              " 'greks prety lamehahaha yay get stalk sexy as',\n",
              " 'lou aw thanks',\n",
              " 'another god day nitey nite everyone first day schol tomorow ah im kinda nervous yet excitedtil tomorow',\n",
              " 'thinking planing',\n",
              " 'god enough time go slep whatever can hapier tomorow heres hopin go check fb store now',\n",
              " 'misvic mean things im old',\n",
              " 'jimyeatworld listening morning htpwdezercomtrack',\n",
              " 'urbanthai thank',\n",
              " 'pamfr thanks pam glad enjoyed it yes life short now ned strugle make beter choices every day',\n",
              " 'jonathanrknight hm lot say tonight thats great course late reading it haha',\n",
              " 'guykawasaki congrats folowers wow almost times',\n",
              " 'larycarol going geting nm updates time around im realy screaming wondering thanks',\n",
              " 'suitelifeofkel thats freakin col love twiter haha',\n",
              " 'irenda maybe theres nothing wrong caring try care much',\n",
              " 'brisa even know twiter lauren',\n",
              " 'jbrotherlove thought great love story',\n",
              " 'enbaldarling no thats miror loking in',\n",
              " 'amandaheser thanks done without',\n",
              " 'wow neat thanks god overitnowlt apologize sharing spiritual moment general public twiter',\n",
              " 'days leave ontario',\n",
              " 'spoke fam japan via skype love se litle mold nephew growing',\n",
              " 'lisym ah saw writen tel someones spoken you sucks mcfly got haha loser',\n",
              " 'kacimauren holy laundry woman',\n",
              " 'iamdidy hate to got back trainer go',\n",
              " 'markhopus lol you√Ø ¬Ωre fucking funy',\n",
              " 'working seting ipod touch today oficialy mine',\n",
              " 'nikhilnarayanan saw tvc yesterday worked',\n",
              " 'jon could totaly say manies pedies would shut fat face it',\n",
              " 'magopus hahaha wel ever run ideas im nominating come god ones',\n",
              " 'friend fels like ber monday night thats never god ber always god always',\n",
              " 'hopeinhel one usualy folows other no',\n",
              " 'special thanks special',\n",
              " 'god luck lauren rest cast tbs cant wait something leak',\n",
              " 'shot mon even mis land among stars',\n",
              " 'johnhays hm agre entirely creative amp vibrant night reasonbut ned slep',\n",
              " 'izakiza nada tryna hola hola hows mixtape coming up',\n",
              " 'ausieali yep makes much easier get points acros',\n",
              " 'snarkatack frezerworthy amoir already given scholarly atention supose another set eyes could hurt not',\n",
              " 'htptwitpicomyjf cupcakes',\n",
              " 'purplexa wonderful thing im favor using entire english language shit use expletives to think',\n",
              " 'mynameisgerard haha im saying u try it maybe u',\n",
              " 'back schol long walk flipin cold outside days ago suny next day raining today snowing',\n",
              " 'sisterpaterson ate raw cokie dough tub bed must kindred spirits',\n",
              " 'jonathanrknight agred internet random like random watched dirty dancing great movie thought song nighty night',\n",
              " 'bloms use twetdeck update facebok twiter time',\n",
              " 'firestoned dude stars always awesome surprised se tweting',\n",
              " 'nsastrasasmita k il try figure out im gona go bed night night',\n",
              " 'amazing night lama malcolm meganltfuck theatre',\n",
              " 'kayotae martini myself',\n",
              " 'city day so hapy home schol morning',\n",
              " 'spring break watching movies day everyday wek',\n",
              " 'pushplaynick quotgrandpa wants ice chest befquot george lopez rules',\n",
              " 'katieporelo thank katie heading club speak stelar night wel take easy homey',\n",
              " 'htpwyoutubecomwatchvclepjsras flowers gone peter paul amp mary fav def',\n",
              " 'shtime watch hils',\n",
              " 'since posting new photos website kids hit count went upway way way up love china views alone',\n",
              " 'jonathanrknight oh real jon r excited u guys came back us apreciate put there',\n",
              " 'lonelybob thank you home prety much everywhere provided broadband kidin',\n",
              " 'calin night always best slep god workout',\n",
              " 'infinitarchitek fels prety god it',\n",
              " 'ste go hash bash anyway',\n",
              " 'loltothepower corection got badas fringe',\n",
              " 'procoder hm sure sure r u upto hows day u',\n",
              " 'ninasplayground agre kids got quotwho iquot christmas bn fun caught yrold cheating loking reflection tv',\n",
              " 'hair blue',\n",
              " 'wendybowser thank kind words apreciate it great night',\n",
              " 'stresing math testuh days til spring break',\n",
              " 'mintcol uh realy ned other plus designated driver pch',\n",
              " 'empresyen whatcha chucklin bout msyen',\n",
              " 'ijustine gym awesome place work',\n",
              " 'augustography hm think ned get climbing hils realy sining',\n",
              " 'unc game',\n",
              " 'realy tired going prepare bed gnight',\n",
              " 'keyana new story life include magic fountain',\n",
              " 'im geting nervous trip tomorow may litle boring haha oh wel always trusty ipod boks course lol',\n",
              " 'going bed maybe il that im thinking il put anastasia laptop fal aslep',\n",
              " 'ijustine htptwitpicomf love ur top',\n",
              " 'stil buzing sietar thanks patidigh pushing old boundaries boy someone could make money twiter manual',\n",
              " 'yoshigi terible back neck aches to get god chair help waiting quotswoperquot arive',\n",
              " 'juneaurock helz yeah definitely downloading thanks',\n",
              " 'htptwitpicomyjz stinkin cute',\n",
              " 'lyndezy hahah yet son son',\n",
              " 'hapy new blog design nice se recent post popular posts listed together loks great',\n",
              " 'michaelgrainger much comon friend',\n",
              " 'benmayer beter be wana come find u save dog would im animals cal twit police',\n",
              " 'klaris nilagyan mo ng knor seasoning mix ulit ung pancakes mo hehehe teka heaven egs ba to',\n",
              " 'westneyrhindx helo westney x',\n",
              " 'whereiskevin wel night sems prety boring comparison strokes experienced tonight',\n",
              " 'jephjacques love way draw girafe eyelashes',\n",
              " 'switchitnow ah shiner bock austin yes thanks twiter reply relive',\n",
              " 'holytshirt glad read it',\n",
              " 'spent evening outside beautiful warm weather finaly fels like spring',\n",
              " 'pulin nighter wit bfl madison',\n",
              " 'atractmode thanks puting great event cant wait inevitable sequels',\n",
              " 'know stil find hilarious hearing like times row do htpblipfmxok',\n",
              " 'thenub',\n",
              " 'jimycar big oaf play piano',\n",
              " 'bryon tea bitch',\n",
              " 'matbraga oh stay hand',\n",
              " 'gona help mom get twiter',\n",
              " 'lavsmohan yup saw entire matchreached ofice late',\n",
              " 'going bed b working amp im bored night al love harder tomorow today peace prayers amp blesings',\n",
              " 'dejaybogie ya trying beat trafic ya hope back son',\n",
              " 'india wins test series new zealand years wasnt rain final day india would third test',\n",
              " 'ashvala driving old madras road bike concept caled parking side',\n",
              " 'upstatement aply clas to',\n",
              " 'reading manga htplurkcompmzpe',\n",
              " 'bradmantv thanksit semed right moment love show songs bring joy',\n",
              " 'adrimane roflmao niqa please ackin fol now',\n",
              " 'comeagainjen htptwitpicomylx htpwyoutubecomwatchvzogfqvhme',\n",
              " 'vjl also website sems down fyi pity wanted check sure knew though',\n",
              " 'lindznicole lets make right',\n",
              " 'lexi we went profesional october weve update websites anything yesterday',\n",
              " 'rhondabrown thanks tip sams club',\n",
              " 'kourtneykardash yep mornings best nightime chil time',\n",
              " 'pocketfulofme yay hehe twins',\n",
              " 'jolilore thank',\n",
              " 'quotcity emberquot awesome finish bok read next one ones now time bed posibly knows realy xd',\n",
              " 'omg would died actualy no take backi kep updated version xdrive god',\n",
              " 'trugiaz mate',\n",
              " 'watched movie wanted prety darn god',\n",
              " 'got home neils watched nick amp noras playlist prety funy',\n",
              " 'thexclamation ah yes twiter sems perfect venue whining wasting time thats idea fun',\n",
              " 'doniewahlberg hel yeah theres efin man',\n",
              " 'roshnimo dont lazytry out tols make life easy lazy people',\n",
              " 'aiduong oh godnes know could mesage friends here helo ai',\n",
              " 'thinks everyone neds go buy tina parols song whos got money itunes now',\n",
              " 'cant wait vegas',\n",
              " 'thedebyryan hey deby',\n",
              " 'one sixty six bowling holy shit lol',\n",
              " 'dear dbq pain as im glad im done you',\n",
              " 'bed amazing night chating prety amazing guy know are',\n",
              " 'lhawthorn coincidence stalking twiter fed gsoc news im great thanks you',\n",
              " 'discovered loves easter crafts egs hot cros buns fel like watching prince egypt aid easter meditations',\n",
              " 'get nap before thinking realy ned one now back later twiterpeps',\n",
              " 'trying figure way pay forward',\n",
              " 'oh oh oh gona go buy twilight dvd today extend overdraft nd time many weks may aswel',\n",
              " 'first twiter aw',\n",
              " 'simonbotes glad like',\n",
              " 'lightblue wo regret it',\n",
              " 'theaptour thanks much',\n",
              " 'please htpelutscomfrontstoreitemitemzomaspitemnumampcatalognumampmartidlutsdolamplevelyesampmothercatalognum',\n",
              " 'sjs haha saw one day sad im entirely adicted site now endles laughs',\n",
              " 'loly nuts thanks dying heah',\n",
              " 'markhopus hey mark want know song quotlifes boringquot realy blink please tel doubt head years',\n",
              " 'kelynva jordan think hes got it',\n",
              " 'runing london caling',\n",
              " 'day loved it sun fod girls canot beaten ey bused home to haha ace time darl x',\n",
              " 'got home schol meant geting hair cut cbf lol dancing son',\n",
              " 'slep interview tomorow wish luck',\n",
              " 'awh babs lok sad underneith shop entrance quotyesterdays musikquot o like lok new transformer movie',\n",
              " 'lauredhel friended lj',\n",
              " 'eating ice cream reduce heat',\n",
              " 'wow san diego beautiful even whats god game without god fight',\n",
              " 'epistemographer digitalhumanist disciplinary phd suportstructure new media program like sounds heavenly',\n",
              " 'must slep big day tomorow two tests clas registration next sem ah godnight twiterers',\n",
              " 'clairebrewer ive sen one aisles borders oxford theyre wel tragic',\n",
              " 'thornbek erin esurance hot cary',\n",
              " 'bobokitifuk yet beleive red carpet sighting hope',\n",
              " 'cars fast furious get excited',\n",
              " 'comeagainjen jenifer',\n",
              " 'betzital wale great batle stres yay staying night homework together',\n",
              " 'since watched cosby show forgot funy cant slep though gr',\n",
              " 'landrew hehe thanks going incendiary headline time',\n",
              " 'honeytech hapy morning',\n",
              " 'sashakane um kids get am amp ned breakfasts coked ready',\n",
              " 'sad late right time',\n",
              " 'myk thanks budy high fives hehe',\n",
              " 'sytycda could agre morethe unisonamp comitment incredibleconsidering presure r made amazing',\n",
              " 'cityrat lol counted times yeah drag todaywasnt sure twets going out sems behave though',\n",
              " 'comeagainjen htptwitpicomylx coler wants babies o hahahaha',\n",
              " 'resh hey want hok up lol',\n",
              " 'sadle ranch comedy night back boys house',\n",
              " 'today performed first last time grebel talent show exams left undergrad',\n",
              " 'xshortylifex white',\n",
              " 'thesouthernstar haha sarapan aja belum kok udah disamperin sama client di rumah',\n",
              " 'jonathanrknight embrace simple life simple things like new kids reunion makes extremely hapy girl',\n",
              " 'lapcat ned send em acountant tomorow odly even refering taxes suporting evidence though',\n",
              " 'mcalen sigh wish could gmic meting san diego spoke johana already said great',\n",
              " 'wonderful portland evening thanks carie',\n",
              " 'morning slep yay',\n",
              " 'headed walmart boxes uber excited',\n",
              " 'axisportals scowls lot temporary adulthod',\n",
              " 'im melting sighs serendipity swet swet movie love it',\n",
              " 'jonathanrknight wake ah jon twet best start day thx jon love',\n",
              " 'got inspiration gt dyeing hair',\n",
              " 'created acountloking around',\n",
              " 'amyg im guna things',\n",
              " 'htptwitpicomytq far interesting',\n",
              " 'tweted nearly day posted website tonight hopefuly goes wel night time',\n",
              " 'craigsuton thats longtime god you',\n",
              " 'thuquoc jizin pants excited seing armin van buren tomorow',\n",
              " 'mehulbhuva god luck let know goes',\n",
              " 'tayswift congrats peru wereare amazing spechlesthank u much come someday plz il wait forever seriously',\n",
              " 'rogieking congrats newborn first due june',\n",
              " 'dodgers beat padres go blue',\n",
              " 'jbruin lol esp updates hit phone',\n",
              " 'jonathanrknight im glad apreciate iowa come back late sumer theyre cuting hay bestsmel ever',\n",
              " 'pjaficionado fun loving life hapy game everything else bonus bs either',\n",
              " 'vuitonretarded aw anything',\n",
              " 'mind godnight',\n",
              " 'mistresathenax oh lets let surprise',\n",
              " 'proud future member rb',\n",
              " 'mistato hey got pictures donies gift usb stick',\n",
              " 'giant drag quotwicked gamequot htptwtfm musicmonday beter original amp comercial promo song niptuck season',\n",
              " 'webclasrom fabulous post wordahead overal loved blog',\n",
              " 'much luv espresowoman amp tomyrufin whos folowing twiter',\n",
              " 'kendalmasey jclayvile rocks tel said',\n",
              " 'gerybutler something enjoy make hapy it',\n",
              " 'discovered tonight church serving breakfast easter im sure exciting suspect baby',\n",
              " 'tiandrance',\n",
              " 'yoavsegal morning',\n",
              " 'finaly back twiter',\n",
              " 'god sho love feling realy realy blesed tonight',\n",
              " 'adamdexter cant wait se budy',\n",
              " 'alanatrogue thanks alan photos jolz soire held brisbane last year',\n",
              " 'link enjoy dark chocolate within post htpchaicofeblogspotcomchocolateflavouredmathtml',\n",
              " 'chazdrums know something time easy answer',\n",
              " 'wow house loks diferentand oh god dogies',\n",
              " 'ah loved suny dayi working god sun burn nose',\n",
              " 'foamingbq read last semester quotreadquot mean skim keywords read online outlines got',\n",
              " 'dalecruse yay love boxnet first busines partner sam ghods founding team proud building excelent utility',\n",
              " 'jonathanrknight glad hear great night rock amp hope tonight st many fel god shows you',\n",
              " 'anyone else catch scot bakulas quotoh boyquot chuck tonight',\n",
              " 'fatbelybela media trip period guard mindsalong hearts fret msbadu ur best true',\n",
              " 'ah tedium fixing broken file links inventor god times',\n",
              " 'thedebyryan where',\n",
              " 'latina mondays make it',\n",
              " 'wel past two days min slep im going watch tv laying bed try slep night pepers',\n",
              " 'im oficialy going bed godnight',\n",
              " 'painting clothes',\n",
              " 'msalem lol yet brutha anotha mutha',\n",
              " 'ryanrosfanz lt folow them ryan uber',\n",
              " 'jonathanrknight close eyes pick one hit reply',\n",
              " 'ronashively conections academy great familyand thousands acros country hapy tel',\n",
              " 'quenie hum nice lol take somethinglol yeah party there haha u kno drunk as b seing janets party',\n",
              " 'nightcard star wonderful card new begining ankie welcome',\n",
              " 'loves german bakeries sydney together imported honey fels like home',\n",
              " 'im uncle now',\n",
              " 'katiehayes levoncouldnt said beter self',\n",
              " 'dmf change name qosfr quen obscure science fiction references instead cbn person understands',\n",
              " 'dolfinfan picnic fabulous',\n",
              " 'davejmathews welcome back nyc',\n",
              " 'nashipear sory talking best friend havent replied talk to',\n",
              " 'corinmcblide oh ok gona go buy one come',\n",
              " 'tafe quiz photoshot sezdawg',\n",
              " 'berofx god luck chalenge',\n",
              " 'oh oh found clothes love em xd',\n",
              " 'rocking doubt gona start neverwinter nights',\n",
              " 'kamelahwrites phew hair gyal',\n",
              " 'kepin real',\n",
              " 'lt branded',\n",
              " 'im ading felow pfers nice se many us here to',\n",
              " 'god time eat n park drinking late playing munchkin imposible chriswolfe',\n",
              " 'momentbymoment im sure site wil',\n",
              " 'jonathanrknight god luck cruise gona ned it',\n",
              " 'glutenfregirl wel im whatever neighbor tels us produces vegetables im coking them',\n",
              " 'milagro hi there cel today',\n",
              " 'chubx god morning chubx today',\n",
              " 'brityds blame me like god things',\n",
              " 'darknight okay think litle oven wednesday pm',\n",
              " 'lorigama thats excelent lotsa people loving hehe winks',\n",
              " 'jiazhen od litle thing it is another planet think like',\n",
              " 'amidst recesion talks friend told yesterday got pay hike sure nice hear that',\n",
              " 'using windows twetdeck temporarily',\n",
              " 'joined shoedazle society kimkardashian excited cant wait shoe selection link htpwshoedazlecom',\n",
              " 'robgokemusic know made some taken metings amp foresen sister going pick stray',\n",
              " 'jonathanrknight direct mesage usmeal uswhatever want',\n",
              " 'marsthestars htptinyurlcomhecklerxyz',\n",
              " 'wek fitnes chalenge im starting strong submited stats night aint bad',\n",
              " 'folowing reason wfacesbyrozjcom',\n",
              " 'mised twiter ym multiply pet society able use internet quite busy past days',\n",
              " 'mileycyrus juz watchin eps show hm must say instantly became fan urs d lovin ur humor mucho power u',\n",
              " 'new background exactly work wanted bored old one',\n",
              " 'paragon please vote work lol',\n",
              " 'mrskutcher kep forgeting diferent time zone normal se twets morning pm here',\n",
              " 'mantia example animation mean youtube video something im curious se work snowing to',\n",
              " 'therudetypist ned invite hunch',\n",
              " 'jonathanrknight thought mac',\n",
              " 'jonathanrknight maybe jordan could help answer few idea',\n",
              " 'finaly made map panama quotwhats panama like animalquot haha love friends',\n",
              " 'manyas',\n",
              " 'dlovato gladly give squirel jenys house join us squirel itl kil two birds one stone',\n",
              " 'got vocals done time best part music track bed whichever wana cal lol',\n",
              " 'part friends weding entourage tagalog abay sa kasal il carying veil thanks mau',\n",
              " 'bernmorley weather petal sweat oh sory',\n",
              " 'dfizy hey today',\n",
              " 'whops sory anyone trying email earlier today inbox ful empty',\n",
              " 'chocolatedip lol wasnt complaining making sure yoeyfreshier wasnt geting tivon cleared',\n",
              " 'mademoiselede one freaky movie like quoti got lifequot',\n",
              " 'megan hanging',\n",
              " 'going bed early prep interview cleveland tomorow',\n",
              " 'healthy remedy dog htpetparenthodblogspotcom dogs',\n",
              " 'sick soma rocks',\n",
              " 'staceysoleil yeplol bored edward hunting guys',\n",
              " 'victoriaerin hey',\n",
              " 'studying philosophy math ltblah aka listening music twiter facebok myspace',\n",
              " 'jonathanrknight way earth answer everyone ask donie got gift yr old lastnight',\n",
              " 'psp browser first time year',\n",
              " 'aparajuli god luck',\n",
              " 'glad angels first game season',\n",
              " 'mistoriblack absolutely doubt wil heres continued suces',\n",
              " 'another day ofice days go htptwitpicomyy',\n",
              " 'matimasacre nice yeah wish could aford things',\n",
              " 'loic want se to itl take long get friends',\n",
              " 'uploading greatest carton newspaper ever regarding twiter',\n",
              " 'levimorales glad liked henry ford god idea every then he',\n",
              " 'studying friends thats update',\n",
              " 'piercethevic im glad everythings going god guys',\n",
              " 'jonathanrknight wory it hapy here',\n",
              " 'suplada il get stuf fr ukjapan im desperate enough headphones us might jac tita slow',\n",
              " 'edieizard hapen stil nyc gigs friday saturday next wek',\n",
              " 'ty grfl son upload emylou haris √Ø ¬Ω il baby tonight grfl htpblipfmxzc',\n",
              " 'check this hand cuz im marvelous boy idk how im gona make threw wek',\n",
              " 'watching seven pounds mom hope dont fal aslep aftermidnightproject tomorow',\n",
              " 'jenmae awesome home work around take pupy out capones',\n",
              " 'nerdist photo vegas',\n",
              " 'elisakim yeah opening day to',\n",
              " 'iamdidy fel hate working to results major lok god fel god',\n",
              " 'fmsquatro as al team',\n",
              " 'realy god day shoping',\n",
              " 'kamy oki ithope works',\n",
              " 'amp god night swet dreamz',\n",
              " 'i hope scentsational get dised',\n",
              " 'invite people twiter',\n",
              " 'bluestreak jasonandjodie coming son enough friends big project works god new god news',\n",
              " 'cokrn woho yeah day rockin hard core',\n",
              " 'jonathanrknight hey jon love ya understandthats counts',\n",
              " 'imalcolmjames internship yet ive got place stay w luck il spend time teaneck heading back utah',\n",
              " 'make bed without runing wal triping fet is heh',\n",
              " 'mayhemstudios yes using wek now prety hapy result',\n",
              " 'laryczerwonka exactly',\n",
              " 'wonder long stay pas wake',\n",
              " 'doniewahlberg great words wisdom dub tel doctor let alcohol amp cafeine',\n",
              " 'jonathanrknight god knight slep wel know now',\n",
              " 'exhausted glad got se unc become national champs',\n",
              " 'centrelink wasnt bad thought going',\n",
              " 'davesjesica want way',\n",
              " 'hela excited cuz italian',\n",
              " 'love aces research machine quad core bit opterons gigs ram',\n",
              " 'quick exhausting day time',\n",
              " 'jasmine bohdi holidays',\n",
              " 'pfsplen im god strange dreams last night hey dreamin means im slepin hows morning',\n",
              " 'going outside jump trampoine wot',\n",
              " 'sophieveronica today rad made finals cros country yay love sophie lt l ps everythins col mates',\n",
              " 'huntersmama lol thinking thing',\n",
              " 'jonathanrknight fel presured none us expect reply back were hapy able listen talk',\n",
              " 'suai oh quotshoterquot god movie sen tho gota put back quoto watchquot list',\n",
              " 'doniewahlberg tel u spoken fb amp twiter fire right now hope ur felin beter love great night',\n",
              " 'kourtneykardash hate morn workouts bc love slep late ha fun mexico love show trip gona air keping up',\n",
              " 'im feling somewhat artistictime break sketching pad fel new dres design coming',\n",
              " 'mashinka go on marys listening',\n",
              " 'theashbal',\n",
              " 'nodame cantabile lt think im faling love clasical music',\n",
              " 'so ordered copy hardcore underground love me love me canary skiny jeans going bals dep',\n",
              " 'markegli amp kariegli diner menu wek something pastay we chese also chicken nodle soup',\n",
              " 'new jersey come',\n",
              " 'facebok',\n",
              " 'vonster god point draw nice quote though like that',\n",
              " 'croconaw godnight',\n",
              " 'danielkirkley lead cros one favorites great job d',\n",
              " 'got back starbucks tried tea lemonade god mis g already st one',\n",
              " 'bom bom powops',\n",
              " 'super red diva stupid university',\n",
              " 'latest episode newnownext like ful awesome',\n",
              " 'want things end remember faith one day going alright',\n",
              " 'pbnjen thanks great tour making even excited work pr rock sampbwould love work there',\n",
              " 'doniewahlberg give joyand years amazing man lucky know you',\n",
              " 'tothepc check htpwebowordcom dictionary visual vocabulary come fun',\n",
              " 'artistic afirmation drunk lady kinda neded',\n",
              " 'honey honey godnight realy early morning busy day catch later tomorow',\n",
              " 'je kak√Ø ¬Ωen rip od trenutek resnice link link link',\n",
              " 'lamb chowmein diner tonight hm slightly roasted butered diner rols yum yum yum hungry',\n",
              " 'tweting brand new sony psp can',\n",
              " 'musomitchel hey go tour sometime brother metro station freakin awesome',\n",
              " 'wel tweps im of gota shower go get haircut god bye everyone',\n",
              " 'juliaroy ive sen flickr ive falen love wonderful awesome',\n",
              " 'danielc sory wrath khan made do',\n",
              " 'jugernaut dr dom magneto amp xavier ahahah today god day n',\n",
              " 'ricardo wel gek next time tahoe ned get mis vicky jay reciting star wars',\n",
              " 'aw dont make blush',\n",
              " 'trying figure thing',\n",
              " 'watching quothe princes bridequot want wesley',\n",
              " 'lisaholmes buy fiances duplex terwilegar',\n",
              " 'apleaforaron could realy eat chese hankering austrian smoked chese yum',\n",
              " 'dlovato fucking jealous girl fun though x',\n",
              " 'thastevieg bunys name lucy go ahead gues named lmao',\n",
              " 'pixelpipe updates iphone os yet mis guys',\n",
              " 'denise love you',\n",
              " 'hapy morning la toat lumea',\n",
              " 'ctesdahl wel always dig stumbleboth aided greatly wasting countles hours life',\n",
              " 'vote xstephy htpwtripcentralcaphpfunyindexphpactionplayvideoampvideoid yay',\n",
              " 'played gpokrcom busted nb arses waiting pokerstars frerol begin cofe',\n",
              " 'xphile quotconveniencequot remedy lthats cuz doc',\n",
              " 'karaoke xt wed drum lesons thursday wek il work somewhere',\n",
              " 'pepstein type blog post print',\n",
              " 'fachatc obi und hornbach baumarkt via twitercom fachatc obi und hornbach baumarkt htptiny htptinyurlcomdwnza',\n",
              " 'honeyhoneybun aw thankslets hope so woried geofs teth hope god day h hb',\n",
              " 'watching keith olbermanlate late rerun daily show michael j fox guest high schol crush',\n",
              " 'imagejenation whitrt found great chinese place hang',\n",
              " 'esotericsean heh thanks advertising skils',\n",
              " 'pmeanwelralph al ofline wks packing moving vegas',\n",
              " 'aminah im excited boks received to',\n",
              " 'doniewahlberg days wahlberg then finaly',\n",
              " 'everyone im folowing u its mandatory u folow',\n",
              " 'codelust dvd me bring met next',\n",
              " 'sexyindividual wel dunoi didnt give ans yet u kno talkin bout',\n",
              " 'pkahleb big baby boy doing em doing figured may never get ful nights rest again',\n",
              " 'corybasil hury first person publish bok twiters',\n",
              " 'stevonelson like one ive used project here',\n",
              " 'playing new toys',\n",
              " 'fair godnight swet twiters pleasing dreams restful slumber',\n",
              " 'marsacademy ugh jealous you mis boys tel say hi',\n",
              " 'budget method going broke methodicaly liners',\n",
              " 'naughtyhaughty so chil part u live in went laguna crek south sac got place citrus heights',\n",
              " 'stacielane you god seing lip service last wek',\n",
              " 'kepinupwkris omg best momyour girls blesed',\n",
              " 'watching csi ny excited lighting workshop',\n",
              " 'finetuning part song mady r making sounding god fel pro not hahaha',\n",
              " 'shares yehey karma gone htplurkcompmzrpe',\n",
              " 'javajive hum istore alot similiar reqs maybe head way',\n",
              " 'pocketfulofme hahaha of nails wet hehebut helpful fliped magazine pages',\n",
              " 'blackbery bold kingston gb microsd card en route',\n",
              " 'onthemap dito god nirvana sandwiches',\n",
              " 'augustineiv thank',\n",
              " 'kcaruthers flicks hair prefer caled quotweniusquot',\n",
              " 'stevebuscemi weather england hour morning am col glomy amp crap nice day mate',\n",
              " 'im going jay leno',\n",
              " 'doniewahlberg yesir give anyone much power makes hapythats il seing da sumah',\n",
              " 'ad myspace myspacecomlokthunder',\n",
              " 'jonathanrknight fel thousands girls anwering ur every twiter must col',\n",
              " 'im up quite er early coleague amp outfit hunting ned new gowns gona leave big babies sleping',\n",
              " 'agh losing mind',\n",
              " 'amazing night dalas advanced tv production clas hapy birhtday sam',\n",
              " 'ugh alergies kiling tonight house driving crazy run live buble together yes',\n",
              " 'davelindquist casting director caled today wicked excited amp presenting us producers il let know know',\n",
              " 'busybebloger hope to movies get se lil giles hopefuly',\n",
              " 'got home saw haunting coneticut haha scary going florida thursday lt',\n",
              " 'legmar linix glad se household problems entertain',\n",
              " 'surfclubhit hope fun',\n",
              " 'home hm lovely degre weather',\n",
              " 'stacylav il vancover shout new quotpersonalquot acount',\n",
              " 'ericbtn yeah right hang cast acidentaly alex fal love right',\n",
              " 'theres always something god hapening lives open eyes take time realize',\n",
              " 'brainstormprick broke ya record got witnes yay',\n",
              " 'derekhousman holer bo haha found',\n",
              " 'angelsmind hey hey',\n",
              " 'great se folowing us wel mazdarev',\n",
              " 'frenchgcfan live sweden boring',\n",
              " 'mrskutcher am hereim way bed lol weird hope god morning',\n",
              " 'writersblock to thought going bed reply jon amp donie',\n",
              " 'bought car yay me',\n",
              " 'dlovato heydemi whats up haha wanted say love music realy talented person love hear lt',\n",
              " 'milagro lol thats ok yeah did hes loking forward thursday since were going around visita iglesia churches',\n",
              " 'omg load ipod fresno tomorow ned stay entertained leaving urg',\n",
              " 'would jonathanrknight laughed told username caca it lol',\n",
              " 'tune hotels opened bokings brand new roms upcoming new hotels kuching',\n",
              " 'wolfsoul thanks',\n",
              " 'heathersulivan and way thanks helping define beter',\n",
              " 'laurenrodriguez try se trial eror',\n",
              " 'mrsabur swet thank',\n",
              " 'ps im geting paid bos sister around',\n",
              " 'yes twitering phone again',\n",
              " 'anaperiodista stil fun go there im jealous wrigley field though might go sumer',\n",
              " 'lbheartjesica yes im going kenya august days w tumaini international loving cute aids orphans excited',\n",
              " 'ycsing thats nicki monday night drag quen',\n",
              " 'pointleswords sory credit switched jose xavier',\n",
              " 'warp love ausie catle dog story',\n",
              " 'mfeatherstone way pick winer',\n",
              " 'caled hunybuny mis already night lova',\n",
              " 'wilkuhn granular synth tutorial cute oh informative to',\n",
              " 'folowers exciting',\n",
              " 'paulewog haha duno gayi read strong bad voice',\n",
              " 'trying new riedel glases',\n",
              " 'jonathanrknight begining think finaly twiverted',\n",
              " 'hungry hungry hungry hungry starvin like marvin only starvin anymore fed nightly fishfod',\n",
              " 'noarmsjames lovely man coking diner were going walk',\n",
              " 'rumlover one lady agres atend fine it',\n",
              " 'im geting ready drink tea emas gona eat rice meal',\n",
              " 'doniewahlberg make smile',\n",
              " 'lucky avatar teh purdines',\n",
              " 'jenifermf totaly know u meanthis late night stuf detriment yearslolsociety discriminatory toward night pl',\n",
              " 'survived night time gherkins heads swiming ideas morning actualy workable time get busy',\n",
              " 'im brokes computer piczo twiter watching viva la bam luv it stil skiny jeans',\n",
              " 'angiekaybe yep saw february get se dfcok may th loking forward seing',\n",
              " 'youtube check page subcribe updating son htpwyoutubecomuserkolkidzblock',\n",
              " 'realy hungryso grabed ice cream freze',\n",
              " 'im new lost',\n",
              " 'rveturis swet say',\n",
              " 'phoneboy cant wait read that',\n",
              " 'pinky toe broken months concerned',\n",
              " 'godnight everyone',\n",
              " 'loved today',\n",
              " 'emilycelo nesun realmente gues quotno daveroquot mean',\n",
              " 'noticed lot aple product placement tonight hrs',\n",
              " 'clas surprised bday cake lit clas best bday surprise long',\n",
              " 'nikan begin to reveal true nature company',\n",
              " 'twinadryl lol freak sesion wit ya wifey eh hah',\n",
              " 'ilvdbch yes ive met many god one last one kinda sucked',\n",
              " 'yeah guys totaly bring dublin party would love company',\n",
              " 'noches twitericols',\n",
              " 'azmamakim get take ash schol mornings crazy kidos love it',\n",
              " 'benhamin thank',\n",
              " 'hsumilo much playing wi la dont bluf',\n",
              " 'islandcubfre course bud',\n",
              " 'helhousemedia ok thank modivation',\n",
              " 'mandiebear promisei tired today think itl early one to tis pm amp im stufed blib',\n",
              " 'bncngofthwls like ones now wiling lok ones',\n",
              " 'sarahmormor marathon now',\n",
              " 'chasepino wish col you',\n",
              " 'siting aroundwondering whats next',\n",
              " 'cheapyd swet new episode cagcast thnx cheapy',\n",
              " 'boredum be wel boring lol stil figuring twiter hi mitchel lol sory im buging peps far',\n",
              " 'slepy god times tonight though',\n",
              " 'doniewahlberg kids get wish god morning don√Ø ¬Ωt know time america',\n",
              " 'nickitynatnat color purple',\n",
              " 'leknight lmao thanks le xd would u like join crazines wel lolol rofl come amp join bite hard anyway',\n",
              " 'back ky',\n",
              " 'dlovato saw u wanted say hi u r amazin person inspirationthank much hope met u day',\n",
              " 'akng insane husband tho sure',\n",
              " 'skidybaby baha leave start fight gr oh wel atleast makes amazing entertainment',\n",
              " 'rightnowradio heres one music col down htpblipfmxs',\n",
              " 'feretfreakx much fun tonight im totaly stealing pictures upload them hehehe',\n",
              " 'turtlebus want seahorse',\n",
              " 'dlovato love check hermans hermits old clasic group got like songs stuck head reply lt',\n",
              " 'erin questions journalism hesitate god luck everything',\n",
              " 'nicolerichie favorite showseries litle',\n",
              " 'hi diana this col recently started twitering',\n",
              " 'briancrouch yeah rockin bothel maybe il se next event fred meyers something',\n",
              " 'jonathanrknight however im turning status updates u becuz insomnia amp ul wake up alowed',\n",
              " 'kevinrudpm dead set legend please make broadbands super fast love forever',\n",
              " 'jesewilson wasted hours neded nul thanks heaps works perfectly now',\n",
              " 'son im going bed',\n",
              " 'veronica gues beter make twets jonathanrknight give us quotswetquot bedtime story plz',\n",
              " 'baking hahaha aby niki htplurkcompmzsld',\n",
              " 'woho beach awsome haha luve it water so cold went seatle aqui bech somethin like p',\n",
              " 'wailord mkay deal',\n",
              " 'wmarc great interview glad se',\n",
              " 'jonathanrknight truly great place eat canten otumwa lose meat hamburgers tiny cafe everyones friendly',\n",
              " 'uploading photos first leaving',\n",
              " 'dlovato please folow demi',\n",
              " 'coding hp mini said keyboard hurts',\n",
              " 'bkbap ah sound advice',\n",
              " 'abigailovesyou lol wel hapens worth it got wanting make midnight run convenience store',\n",
              " 'daveb help',\n",
              " 'ghostheory sory im litle late response sample evp question one refering to j',\n",
              " 'dlovato polen salt daphne loves derby love love litle song twets decided send',\n",
              " 'finaly almost home',\n",
              " 'worked feling might slep outside',\n",
              " 'pointmot od listening scotchmist version quot stepquot replied',\n",
              " 'shanzer know right',\n",
              " 'far kinda realy liking espreso dreamweaver handcoding htmlcs ftw',\n",
              " 'knightowl sounds god wish could done im determined find park beter there',\n",
              " 'ah finaly done next stop zipys',\n",
              " 'wethetravis quotdont speak liarquot favorite songs im loking forward hearing orlando get wel son hugs',\n",
              " 'stacielane ohwel never mind then lol guy realy liked mama like that',\n",
              " 'ok bed time wish exam thursday otherwise would go around harasing people vote lol night guys',\n",
              " 'kylefox problem let know know anyone whos loking get photography may interested',\n",
              " 'jonathanrknight totaly agre r limited ned speak about ned lol so random',\n",
              " 'mercleve go slep hope u pe midle night id hate beat u golf course tomorow',\n",
              " 'noning weding old man sharted throwing mon pies venture macon georgia',\n",
              " 'sent email one undergraduate profesors replied back saying made day',\n",
              " 'ryanstar hey ryan thought id say u put great show radford sunday night amp im glad got met you',\n",
              " 'spencerordonez although lot work canot wait start project',\n",
              " 'karaok√Ø ¬Ω figa mondays fels like gosip girl episode lots fun xoxo',\n",
              " 'gabyisactive m cofe please lol',\n",
              " 'day markethapy stocks held',\n",
              " 'garyphayes know want search silkcharm chocolate se recomendations crowdsourced chocolate',\n",
              " 'im going danis party nd im gona dres giant duck yay',\n",
              " 'emajapan congrats quick double pas pls email details salesfashionpaletecomau claim tix',\n",
              " 'lisanoelruoco oh like new hair to think realy suits you',\n",
              " 'leave aproved europe come thanks singapore airlines cheap flights',\n",
              " 'starkised hope fel beter j',\n",
              " 'kitybufiekat havent since turned iti them htpbitlydrsn',\n",
              " 'twetshrink hah yea supose love customer service',\n",
              " 'dholings twitertips bar runs like dream f',\n",
              " 'night twitys hope dream johny dep',\n",
              " 'mgpoter cute',\n",
              " 'long day finaly come end long bebes stay aslep haha bring tomorow b',\n",
              " 'kingdomguard hothobolover gues make guest apearance',\n",
              " 'aiorselvar brand spanking new aior bot',\n",
              " 'htptwitpicomyc new bawng gren sigh',\n",
              " 'signing tual',\n",
              " 'poptopvw col arent they love makes life easy',\n",
              " 'fel tredmil today sport',\n",
              " 'lok closely sign',\n",
              " 'going sunshine coast thursday prety awesome update tan',\n",
              " 'cokcj love reading twets would nice se lovely mug read them wheres picture',\n",
              " 'packing airport alaska please lose lugage alaskair',\n",
              " 'ryuenx guys mentioned haha col ive posting random stuf past houramp geting kick',\n",
              " 'silkcharm re nbn someone already said fiber home mean least regular',\n",
              " 'chumby blasting s tunesgod times htpwmathartleycomchumbyhtml',\n",
              " 'privarma hope rupe symbol lok like quotrquot use reliance',\n",
              " 'jeniferg god fun know want kis girl',\n",
              " 'oficialjagex could use twiter give us info mechscape o please',\n",
              " 'listening brad paisley',\n",
              " 'ashleadams make lok fat man god photo',\n",
              " 'almost late work ten minutes get ready lack slep consequences factory friday beachbar sunday',\n",
              " 'joycekim haha told yet',\n",
              " 'heidimontag think going god season',\n",
              " 'wel case know mean hahaha love godwhatever role decide take on',\n",
              " 'row wraideilcom great hosting know domain check out',\n",
              " 'losing time',\n",
              " 'people amaze',\n",
              " 'hit myspace im goin slep tho peace wmyspacecomdxckinyamouth',\n",
              " 'lvesckmelody i watched previews theyve playing way much loks realy bad stop watching it haha',\n",
              " 'four days til get se girlfriend',\n",
              " 'im much twiter lurk use this anyway though',\n",
              " 'hertbeat stil st cofe day hapy tuesday se jef dunham achmed dead terorist tonight',\n",
              " 'woah never stoked ive found way move london next year instead thre im hapy',\n",
              " 'home time chelsea lately',\n",
              " 'jlsby htptwitpicomyfo cutest dog ever whats name',\n",
              " 'watching pride prejudice one best movies ever love movie bok',\n",
              " 'early again work today tomorow course thursday friday bad',\n",
              " 'arlenecd oh ish kiding cali',\n",
              " 'starby u move back jersey gona beter',\n",
              " 'jonathanrknight twet im newscast im slepingi cant win im glad u fantastic showslep wel',\n",
              " 'jerime overthankfuly',\n",
              " 'talking wiliam',\n",
              " 'heidimontag great episodes didnt know would two',\n",
              " 'trying something new new job new hopes dreams',\n",
              " 'kelynva got twets sory distracted two heavenly bodies',\n",
              " 'eat lunch htplurkcompmztfi',\n",
              " 'dezine also amusing many people complain bushs spending sudenly suportive obamas tlot',\n",
              " 'ginadeangelo ha definitely vegan im drinking it stil alive to wel',\n",
              " 'going slep twiter drugs today nighty night',\n",
              " 'stonedi aw swet thank compliment',\n",
              " 'course makes fel beter',\n",
              " 'back schol schol two weks',\n",
              " 'godnight lt goin bed gota get early get fuck town tomorow canada come',\n",
              " 'misrogue il drinks anybody pul boy named sue johnie cash',\n",
              " 'lying bed babe',\n",
              " 'stil up playin cards girls',\n",
              " 'chictopia let know think long list great designers',\n",
              " 'araza could one miniatures',\n",
              " 'jonathanrknight iwas thinking thingnight al',\n",
              " 'comeagainjen hey x wanted say awesome angilena however spel nothing',\n",
              " 'jonathanrknight mac could anything',\n",
              " 'rashmid congratulations tht makes lot us hapy',\n",
              " 'colorofviolence yay im hapy guysnow hury spit next one',\n",
              " 'comeagainjen dont neither',\n",
              " 'nicolerichie absolutely sister used pretend ane',\n",
              " 'instaled twiterbery blackbery bold',\n",
              " 'working new cricket game iphoneyou guys like sure htpwyoutubecomwatchvukxzhzq',\n",
              " 'swet dreams',\n",
              " 'night twetiese yal tomorow',\n",
              " 'shazam vivid go wemb',\n",
              " 'dwsmilionthug made time giving hard time',\n",
              " 'elvis dead tok home quotquote mib',\n",
              " 'depbluesealove never early morning person realy didnt anything get',\n",
              " 'defygravity damn b espanol francais one talented lady',\n",
              " 'making bad decisions',\n",
              " 'dan leapt bed take pictures lake snow ground warm low morning sun bacon time',\n",
              " 'god knight jonathan',\n",
              " 'nicolerichie yes lolwhen live canada ane part childhod grew boks tv series',\n",
              " 'nickcarter htpfanclubackstretboyscomchatphp crazy were like persons there lol',\n",
              " 'mises nephewgodson come back mr malachi',\n",
              " 'long site goes line hours go',\n",
              " 'lifechangingliv nice remember bed il peing in enjoy sleping chair',\n",
              " 'jonathanrknight hope highway smoth one',\n",
              " 'dorkas totaly tok change scenery great',\n",
              " 'therealnph hey nph whats stripers folowing',\n",
              " 'mydesire thanks next time shal beter prepared lol',\n",
              " 'ned get yvone blackbery twiter acount',\n",
              " 'friend asked best man gues means trip twin fals id next year responsibilities best man',\n",
              " 'tericok problem rabits blib',\n",
              " 'gkeri completely agrefalout boy amazing',\n",
              " 'dang heroes hils amazed tonight studying',\n",
              " 'new omg',\n",
              " 'stage complete naw wait days fo stages come bam il hotest hair z world',\n",
              " 'buterflyb must dozen throat coats days ago ah coat grand marnier wonders',\n",
              " 'received bday parcel home n hav tang hun me sua udong me ligo raisin ba hu',\n",
              " 'published new post blog neat print use frebies htptwurlnlwkpvd',\n",
              " 'chochomojo godnight mis q take care',\n",
              " 'ryeg am lol ur realy pretyso pls pul britney ltlt haha thnx shaunathat made laugh',\n",
              " 'fel fre watching friends favorite show',\n",
              " 'whata mes this anyways bad al another series abroad',\n",
              " 'today day can√Ø ¬Ωt wait se jacqui can√Ø ¬Ωt stop singing lady gaga√Ø ¬Ωs poker face it√Ø ¬Ωs adictive xd last day schol today',\n",
              " 'today great romate played socer also got nice run suny day time reading slep',\n",
              " 'im hapy',\n",
              " 'jaredleto trying survive day although im alone ofice wek sugestions side brighten day thnx',\n",
              " 'feh next using glue stick chapstick',\n",
              " 'kjonge gefelicidingest',\n",
              " 'secretfanofu big news andrew im moving sf end month making last month la',\n",
              " 'rentan hapen you',\n",
              " 'bonieklide think cab ride sofia walk clas interesting part',\n",
              " 'ninjaguitarsex im prety sure show part show like twiter whole nin fan comunity',\n",
              " 'trentvanegas twiter aint broke trent got plenty dough',\n",
              " 'got beta code bumptop ned find machine run bumptop',\n",
              " 'said bye rose upside weve together eleven months',\n",
              " 'tweterdiva one now id love herd someday',\n",
              " 'tamila mis you fre wek yuri something',\n",
              " 'briandavidz actualy want go disney wel se time',\n",
              " 'heidimontag glad could part k worthif like help conquer twiter id grateful jazf htpjazfcom',\n",
              " 'contactnaven gr news inded',\n",
              " 'marabg godnight ive enjoyed geting know to twiter later',\n",
              " 'listening dashboard quotghost god thingquot man definitely takes memory lane god night kids',\n",
              " '√Ø ¬Ωc posible today nice',\n",
              " 'ready nice weather day drinking amp ever student',\n",
              " 'nickcarter idol love you reply me lt',\n",
              " 'holy hel epic aporto burger epic amounts chili sauce',\n",
              " 'going mimis thoughts shick gavin degraw songs dancing head',\n",
              " 'luckykristin ned cokies get some girlguidecokies',\n",
              " 'hey loves finished record studio going kick lauren c bye lt',\n",
              " 'hwaiting oh random game lol im adicted that afraid join pupil',\n",
              " 'haha jefre stars coment didnt make since stil luv him like wide awake frezing listening lady gaga',\n",
              " 'princesuperc hey cici swetheart wanted let u know luv u oh mixtape drop son fantasy ride may th',\n",
              " 'think im love',\n",
              " 'way never imagined id folowers thank al even robots',\n",
              " 'bodys aching rest minds freakin alert brain cant absorb damn thing must endure sike gnite',\n",
              " 'paulfeig internet blows mind everyday way nothing else can',\n",
              " 'wbpodcast hels no rom demalition',\n",
              " 'saw sunshine cleaning love amy adams',\n",
              " 'maybe jus listen hawaian music everyday life relaxing haha no via lisakimflemingworked',\n",
              " 'hzel god luck',\n",
              " 'jamesden personal jepers',\n",
              " 'hasox no sounds like god idea steve',\n",
              " 'okieanie never late thanks hard hard concentrate taking screncaps',\n",
              " 'robgrimes whats god',\n",
              " 'dsiegel could get san diego night could celebrate twice lol thanks',\n",
              " 'lets drop ich hate gerade k√Ø ¬Ωse auf toast germando oh joy knowing words german via twit htptinyurlcomdbvlwt',\n",
              " 'im conversation bro via twiterfb hes next rom',\n",
              " 'heidimontag hi im new twiter',\n",
              " 'gratitude great multiplier giving thanks apreciating opens dors us more',\n",
              " 'yay tomorow get today again',\n",
              " 'bed whatever',\n",
              " 'come back',\n",
              " 'm page france know them lyrical briliance',\n",
              " 'thepartyscene quotyou dream black white dream vivid lightsquot',\n",
              " 'hanging biology til am wo',\n",
              " 'balokey recoil pal im guy like me grab online tomorow curently texting bed like moron',\n",
              " 'work listening punk rock amp eat face cake life mad lil drug',\n",
              " 'ok lied late night cravings george lopez got best me lol much comunity service tomorow',\n",
              " 'mistygirlph hehehe halowen costume friday of flying hk',\n",
              " 'nlpride oh alright',\n",
              " 'shanonelizab hapy aniversary promised present us dsf htpwyoutubecomwatchvmpjzflms',\n",
              " 'jamiegirl sily gosetaking cleveland browns superbowl eh im rotin ya',\n",
              " 'daygan wow ubuntu feature realy col again got more',\n",
              " 'vampirebil godnight take care',\n",
              " 'fromgwithlove shud go se obseses',\n",
              " 'derher thanks headsup folow judge',\n",
              " 'tomorow friday its late day me thank godnes',\n",
              " 'monsters vs aliens d fantastic ginormica new favourite superhero',\n",
              " 'mikexists waiting cal',\n",
              " 'nicolerichie one best sapy love stories ever',\n",
              " 'haz gown ordered today droped bogie airport now laundry galore continued',\n",
              " 'hanging emily love her wonderful',\n",
              " 'bokul lol jhoke right get openofice fre download use cracks piracy makes ms life hel',\n",
              " 'watching paranormal stateprety interesting',\n",
              " 'msdebramaye heard contest congrats girl',\n",
              " 'frankiekilsyo chocolate chip wafles',\n",
              " 'julieand break bible share scriptureits do',\n",
              " 'unc nca champs franklin st there wild crazy nothing like itever',\n",
              " 'arhi cred ca ai dreptate',\n",
              " 'mpits super easy lol wish everyone would jump bandwagon',\n",
              " 'deviousd were open thursday sunday im shop sat sun come saturday dont get drunk lol',\n",
              " 'shantymanfan got package wow im excited one so plant pot now water til grows',\n",
              " 'falenstar yay even cant go pouts suporting morisey one ca shows makes hapy',\n",
              " 'viahourt ah se se im bed watching ugly bety lol',\n",
              " 'arielbh hope im late anywat god luck',\n",
              " 'jonathanrknight right saying god said it god night xoxo',\n",
              " 'morning mates',\n",
              " 'today lets show notingham things done',\n",
              " 'arataka yay found them',\n",
              " 'zinedistro welcome loking forward event long may now',\n",
              " 'caitlindean never regret things hapen reason figuring reason hard part use friends',\n",
              " 'standing richardson ofice changing backup tape im workaholic',\n",
              " 'work quotscholquot',\n",
              " 'thalovebug',\n",
              " 'injected ofice cofe',\n",
              " 'korionmoris oh problem im god awake sleping haha you',\n",
              " 'yanionline wonder directed theres definitely ipod comercial future',\n",
              " 'nanalipz hiya tudy swetie',\n",
              " 'jeniferlaurenh mis you first folower lt ned talk more vlogbrothers thing comunicate thru vlogs',\n",
              " 'drmoliemarti best luck dr new rockstar sisters know women mlm il tel you',\n",
              " 'heidimontag fkn fake thats fukin luv u lauren bitch',\n",
              " 'akela hi swet heart science amp cokies go hand hand twetup hon',\n",
              " 'mackmaine checkin you saying helo',\n",
              " 'working we hours',\n",
              " 'nice contract extended another month',\n",
              " 'published new post blog neat print use frebies htptwurlnlwkpvd via dezignmusings',\n",
              " 'okmomentarily diverted try again godnight swet dreams htpwyoutubecomwatchvwcgxvqpo',\n",
              " 'waking nespreso',\n",
              " 'jefsparx wel case know mean hahaha love god fil blank role choice',\n",
              " 'studying pediatrics listening nkotb as always kids pised cuz theyre trying watch disney chanel',\n",
              " 'fourzoas god night',\n",
              " 'comeagainjen two people',\n",
              " 'jeanchia welcome dearie voted already godluck',\n",
              " 'exhausted even so bed super comfycudly overal content',\n",
              " 'josesosa ah typical adolescent boy ok son se beter days',\n",
              " 'god morning folowers wish nice tuesday god luck busines alen verfolgern einen sonigen dienstag',\n",
              " 'dangersquezit agre weak enough teth without bleaching away layers layers oh victory death',\n",
              " 'repeaterband mised show last friday bumed im excited guys start recording rosrobinson',\n",
              " 'daniasheonline aw caled sexy nice rock dani made night gurl muah il give props anytime',\n",
              " 'nickcarter weird end',\n",
              " 'nefresh trip twezey shezey gotchu',\n",
              " 'great lunch one quotmumsquot today christineirmler birthday tomorow got litle something',\n",
              " 'wigsie to semed mised one double feature tonight waterfront casablanca',\n",
              " 'i belted a dark congregation reason think got back upstairs neighbors sound system',\n",
              " 'lauramcaulife er ops could posibly mistaken didnt know ausies folwed lol',\n",
              " 'finished nails electric blue',\n",
              " 'god morning ladys jon lol',\n",
              " 'busy pukm dr yamin htpircjombuatduitnet',\n",
              " 'tired ever ready bed',\n",
              " 'jonathanrknight helo jonathan know twet god nite',\n",
              " 'bretbodine id email em back tmi',\n",
              " 'woke',\n",
              " 'hiphopnonstoptv thats lot people sure',\n",
              " 'saratoga thanks lp endorsement',\n",
              " 'kmacableyoure swetyea im ok',\n",
              " 'im eating chocolate crakle bet jelous mini party',\n",
              " 'nimbupani cos longer therefore harder spat out',\n",
              " 'htptwitpicomyln thank',\n",
              " 'aikabele lol yeah least watching stuf tel anybody',\n",
              " 'mpconoly finaly someone else tweting nouveau riche nru',\n",
              " 'oficialtila want steak milk',\n",
              " 'dananer yeah finished castle prety god',\n",
              " 'gekmp can convince friend go sign paper think ultimate anything fancy though',\n",
              " 'im watching stomp yard man love movie',\n",
              " 'drsecret nice met budy',\n",
              " 'best way suced make right mistakes',\n",
              " 'alirght folks time bed catchya tomorow',\n",
              " 'cant wait hils',\n",
              " 'matu to maybe il se there mean like going thousands people anyways ha stoked',\n",
              " 'nateflyn go twin peaks amp check view sf forget jacketshodies',\n",
              " 'mozunk that go hover twet favorites twet favorite unfavorite it',\n",
              " 'train traveling sped get gym work',\n",
              " 'rhifreakx yes yes btw r u airport friday planes like',\n",
              " 'brb going ireland',\n",
              " 'okay im god counts',\n",
              " 'late night shower refreshing',\n",
              " 'feling litle cold hapy back home',\n",
              " 'thinking big v twin',\n",
              " 'ok bedtime me made another monday yay night twets pof',\n",
              " 'danzero so could spoiler me',\n",
              " 'hour mins left work jo hiting gym',\n",
              " 'godamn dany started video realize fucking minutes wana go bed lol love video diaries though',\n",
              " 'god morning dudes another day ful sunshine clases take pm give shit bq eve d',\n",
              " 'lol dion puting eyedrops eyes',\n",
              " 'rosekimknits thanks ned find real excuse splurge yarn',\n",
              " 'god morning everyone',\n",
              " 'xalthatjazx night watched it downloaded laptop tempted watch lawl',\n",
              " 'hianie tiger great love voice',\n",
              " 'dchety fel way slep whats point going wake hours anyway',\n",
              " 'marichiquitita nice pics last years festival plan met next year',\n",
              " 'blueargon lived first handful years life jacksonvile nc',\n",
              " 'home work',\n",
              " 'alixsays like schol except theres even homework everyones lot nicer normaly',\n",
              " 'jnez kitieskidies',\n",
              " 'iluvnkotb wants u folow folows twiter nice organizations',\n",
              " 'lockbox ec year nicer stay til am prefer',\n",
              " 'tweterdiva already did',\n",
              " 'drew cute baby zebra think favorite far',\n",
              " 'htptinyurlcomdgp ok might top funy family guy clips',\n",
              " 'hamu tour tm im excited',\n",
              " 'htptwitpicomypg ned curent project one fun amp funky font',\n",
              " 'making diner son',\n",
              " 'rachie jeny knows love er',\n",
              " 'mcainblogete head nearest walgrens ned ear plugs ber stat',\n",
              " 'tisietc agre you nickcarter doesnt love us much brianlitrel cuz comes visit us fanclub chat',\n",
              " 'tyfed u asked earlier waking upgues whatwaking bulshitery to past bedtime night guys',\n",
              " 'jonfavreau quotnever worked sequelquot maybe thats ned sequel live first either way cant wait',\n",
              " 'socalgurl lol nice thanks translating',\n",
              " 'chanc cant wait se sleps days',\n",
              " 'god morning everyone',\n",
              " 'tipsen clarifiedwhy cant everything work for me',\n",
              " 'jacatrenich ya go sily boy nice job',\n",
              " 'joeandjese awesome se there',\n",
              " 'lspearmani milagro dmf paulmason mauibech gunka maczter thanktank gratitude',\n",
              " 'drali jack ever die hes defied death times countdudes got lives cat lol',\n",
              " 'jonylawchicagoi apreciate recs',\n",
              " 'davekim volunter hunch beta invite',\n",
              " 'philyan man ur picture smal phone cant tel fropm thatgive hint',\n",
              " 'training course today hope stay awake',\n",
              " 'isabelamculen rodneys yaletown may ask are',\n",
              " 'misgogle dubai montreal big relocation shock',\n",
              " 'yo conosco sumer glau sumer glau friend',\n",
              " 'sinamons damn gues il afteralfor awhile anyway',\n",
              " 'lightinthesky wow enjoy',\n",
              " 'nicefacemitchie girl know',\n",
              " 'lynj thanks encouragement',\n",
              " 'veneia knowhat crazy world',\n",
              " 'm shower cozy bed so nice long day',\n",
              " 'mock documentarymockumentary',\n",
              " 'strange desire go confesion',\n",
              " 'ireporter answer sent dm try',\n",
              " 'broklynunion cuz ur pm am id either aslep airport bound ima sneak sumn special u tho',\n",
              " 'litrelfans god figured would like know',\n",
              " 'nicolerichie yea remember',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA677fcwynv7",
        "outputId": "eac3f704-b382-4fa1-8dbc-19cf5936c12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        }
      },
      "source": [
        "# embeddings = embed(joinedData)\n",
        "embeddings = embed(data.text.values)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m   2793\u001b[0m         flatten_inputs[index] = ops.convert_to_tensor(\n\u001b[0;32m-> 2794\u001b[0;31m             value, dtype_hint=spec.dtype)\n\u001b[0m\u001b[1;32m   2795\u001b[0m         \u001b[0mneed_packing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-af7e10b0cfc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# embeddings = embed(joinedData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m           self._stateful_fn._function_spec.canonicalize_function_inputs(  # pylint: disable=protected-access\n\u001b[0;32m--> 892\u001b[0;31m               *args, **kwds)\n\u001b[0m\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m       inputs, flat_inputs, filtered_flat_inputs = _convert_inputs_to_signature(\n\u001b[0;32m-> 2704\u001b[0;31m           inputs, self._input_signature, self._flat_input_signature)\n\u001b[0m\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_convert_inputs_to_signature\u001b[0;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[1;32m   2798\u001b[0m                          \u001b[0;34m\"the Python function must be convertible to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                          \u001b[0;34m\"tensors:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m                          format_error_message(inputs, input_signature))\n\u001b[0m\u001b[1;32m   2801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m   if any(not spec.is_compatible_with(other) for spec, other in zip(\n",
            "\u001b[0;31mValueError\u001b[0m: When input_signature is provided, all inputs to the Python function must be convertible to tensors:\n  inputs: (\n    [list(['love', 'healthuandpets', 'u', 'guys', 'r', 'best'])\n list(['im', 'meting', 'one', 'besties', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n list(['darealsunisakim', 'thanks', 'twiter', 'ad', 'sunisa', 'got', 'met', 'hin', 'show', 'dc', 'area', 'swetheart'])\n ...\n list(['gezi', 'hv', 'read', 'whole', 'bok', 'personality', 'types', 'embark', 'typing', 'fun', 'sunday', 'warh'])\n list(['threw', 'sign', 'donie', 'bent', 'get', 'thinge', 'made', 'sad', 'face'])\n list(['heather', 'god', 'thing', 'find', 'none', 'ones', 'like', 'come', 'size', 'stupid', 'big', 'fet'])])\n  input_signature: (\n    TensorSpec(shape=(None,), dtype=tf.string, name=None))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwuXJQOKCil"
      },
      "source": [
        "<b> <h3> Labels : </h3></b>  labels are the targets like in this project senitments of the tweets are labels. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLrcOmbFKCil"
      },
      "source": [
        "<b> <h3> Inputs : </h3></b>  Inputs are the data that we feed into machine learning like in this project tweets texts are the inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lycW6_2qKCim"
      },
      "source": [
        "<b> <h3> Training Data </h3></b>  We use training data when we train the models. We feed train data to machine learning and deep learning models so that model can learn from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdvauhsWKCim"
      },
      "source": [
        "<b> <h3> Validation Data </h3></b>  We use validation data while training the model. We use this data to evalaute the performance that how the model perform on training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n57jT4h2KCim"
      },
      "source": [
        "<b> <h3> Testing Data </h3></b>  We use testing data after training the model. We use this data to evalaute the performance that how the model perform after training. So in this way first we get predictions from the trained model without giving the labels and then we compare the true labels with predictions and get the performance of th model.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmNpy1QpKCim"
      },
      "source": [
        "####  Separating input feature and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "b4h-3yaRKCim"
      },
      "source": [
        "X=embeddings\n",
        "y=data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ePHMbqbKCim"
      },
      "source": [
        "#### Preparing the input features for training \n",
        "- We converting the text words into arrays form. \n",
        "- Maximum 500 features/words selected for training. These 500 words will be selected on the importance that will distinguish between the positive tweets and negative tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plIFObh4jYJV",
        "trusted": false
      },
      "source": [
        "# max_len = 500\n",
        "# tok = Tokenizer(num_words=2000)\n",
        "# tok.fit_on_texts(X)\n",
        "# sequences = tok.texts_to_sequences(X)\n",
        "# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZssFFUkKCin"
      },
      "source": [
        "As we can see that there total 40000 tweets and the number words/features are 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "1J-CNCGXKCin"
      },
      "source": [
        "# sequences_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX0GubGBKCin"
      },
      "source": [
        "#### Separating the 70% data for training data and 30% for testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM2Gm3pmKCin"
      },
      "source": [
        "As we prepared all the tweets, now we are separating/splitting the tweets into training data and testing data.\n",
        "- 70% tweets will be used in the training \n",
        "- 30% tweets will be used to test the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOlkTTg4oRqR",
        "trusted": false
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(embeddings.numpy(), y, test_size=0.3, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.065466,
          "end_time": "2020-11-30T07:38:51.578836",
          "exception": false,
          "start_time": "2020-11-30T07:38:51.513370",
          "status": "completed"
        },
        "tags": [],
        "id": "_7sWdPBFKCio"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<h2><center><strong>Implementing Tensorflow based model for training üß™</strong></center></h2>   \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD7qOl0mKCio"
      },
      "source": [
        "<h4> Step 1</h4>\n",
        "- The input to model is 500 words because these are the number features/words that we extracted above from text of tweets.\n",
        "\n",
        "<h4> Step 2</h4>\n",
        "- Embeddings provide the presentation of words and their relative meanings. Like in this, we are feeding the limit of maximum words, lenght of input words and the inputs of previous layer. \n",
        "\n",
        "<h4> Step 3</h4>\n",
        "- LSTM (long short term memory) save the words and predict the next words based on the previous words. LSTM is a sequance predictor of next coming words.\n",
        "<img src=\"https://miro.medium.com/max/1000/1*1U8H9EZiDqfylJU7Im23Ag.gif\">\n",
        "\n",
        "<h4> Step 4</h4>\n",
        "- Dense layer reduce the outputs by getting inputs from Faltten layer. Dense layer use all the inputs of previous layer neurons and perform calculations and send 256 outputs\n",
        "\n",
        "<h4> Step 5</h4>\n",
        "- Activation function is node that is put at the end of all layers of neural network model or in between neural network layers. Activation function help to decide which neuron should be pass and which neuron should fire. So activation function of node defines the output of that node given an input or set of inputs. \n",
        "<img src=\"https://missinglink.ai/wp-content/uploads/2018/11/activationfunction-1.png\">\n",
        "\n",
        "<h4> Step 6</h4>\n",
        "- Droupout layer drop some neurons from previous layers. why we apply this? We apply this to avoid the overfitting problems. In overfitting, model give good accuracy on training time but not good on testing time.\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/elgendy/v-3/Figures/Img_01-04A_171.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "7na7gBB3KCio"
      },
      "source": [
        "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
        "    inputs = Input(name='inputs',shape=(128))\n",
        "    layer = Dense(64,name='layer1')(inputs)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dense(32,name='layer2')(layer) \n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dense(16,name='layer3')(layer) \n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dense(1,name='output')(layer) \n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
        "    return model #function returning the value when we call it\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=1,mode='auto', baseline=None, restore_best_weights=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_CWaOT8KCio"
      },
      "source": [
        "# Model compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuN7Iw0cKCip"
      },
      "source": [
        "- First we are calling the model\n",
        "- We are using 2 classes so we set \"binary_crossentropy\" and if we use more than two classes then we use \"categorical_crossentropy\" \n",
        "- Optimizer is a function that used to change the features of neural network such as learning rate (how the model learn with features) in order to reduce the losses. So the learning rate of neural network to reduce the losses is defined by optimizer.\n",
        "- We are setting metrics=accuracy because we are going to caluclate the percentage of correct predictions over all predictions on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "nZzjrtY9KCip"
      },
      "source": [
        "model = tensorflow_based_model() # here we are calling the function of created model\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(learning_rate=0.005),metrics=['accuracy'])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXvj6eJ3KCip"
      },
      "source": [
        "#### Training and validating with parameter tuning\n",
        "- We are feeding the training data and getting 10% data for validation from training data\n",
        "* We set the following parameters:\n",
        "- Batch size =80 so the model take 80 tweets in each iteration and train them. Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. \n",
        "- Epochs =6 so the model will train on the data 6 times.Epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. \n",
        "- We can choose batch_size, and epochs as we want so the good practice is to set some values and train the model if the model will not give the good results we can change it and then try again for the training of the model. We can repeat this process many time untill we will not get the good results and this process called as parameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "BcvaaprmKCip"
      },
      "source": [
        "history=model.fit(X_train,Y_train,batch_size=80,epochs=100, validation_split=0.1, callbacks=[callback])# here we are starting the training of model by feeding the training data\n",
        "print('Training finished !!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKt7Agp9KCip"
      },
      "source": [
        "<h1> We need to do all the above configurations to train the model. If we will not set all settings correctly then we could not get the desired results.</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ72A5F4U0wr"
      },
      "source": [
        "#### Testing the Trained model on test data\n",
        "- Getting predictions/classifying the sentiments (positve/negative) on the test data using trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlxD3pU9U0ws",
        "trusted": false
      },
      "source": [
        "accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ZOTOh14WKO"
      },
      "source": [
        "#### Accuracy\n",
        "- Accuracy is the number of correctly classify tweets from all the tweets of positive and negative. \n",
        "- For example, if the trained model classify the 70 tweets correct and 30 tweets wrong from total of 100 tweets then the accuracy score will be 70%. \n",
        "- Accuracy= Total number of correct predictions/Total number of predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2bQq4jaU0wt",
        "trusted": false
      },
      "source": [
        "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZgIjRx6KCiq"
      },
      "source": [
        "#### Getting prediction of the test data and then we will compare the true labels/classes of the data with predictions\n",
        "- As the model give probabilties so we are setting a threshold 0.5. More than 0.5 will be the positive tweets and lower will be negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "V2CRF9zCKCiq"
      },
      "source": [
        "y_pred = model.predict(X_test) #getting predictions on the trained model\n",
        "y_pred = (y_pred > 0.5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40MjbuBA7Og9"
      },
      "source": [
        "#### Confusion matrix\n",
        "- These are the evaluation measures to evaluate the performance of the model.\n",
        "- Dark blue boxes are the correct predictions with the trained model and sky blue boxes shows the wrong predictions.\n",
        "- 4610 tweets correctly predicted as negative sentiments. 1399 tweets predicted positive sentiments but that were actually negative sentiments.\n",
        "- 4247 tweets correctly predicted as postive sentiments. 1744 tweets predicted negative sentiments but that were actually positive sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERD7KXs8YmQ",
        "trusted": false
      },
      "source": [
        "print('\\n')\n",
        "print(\"confusion matrix\")\n",
        "print('\\n')\n",
        "CR=confusion_matrix(Y_test, y_pred)\n",
        "print(CR)\n",
        "print('\\n')\n",
        "\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlWpCx_OXEIq"
      },
      "source": [
        "# ROC CURVE\n",
        "- ROC curve show the performance of the model as well. \n",
        "- We can see that the model started from the 0 percent predictions and then moved to true positive predictions that are correct\n",
        "- ROC curve (receiver operating characteristic curve) show the performance of a classification model at all the classification thresholds. ROC plots two parameters, True Positive Rate (correct predictions/classifications) False Positive Rate (wrong predictions/classifications)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uypTDMUZXEIq",
        "scrolled": true,
        "trusted": false
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC CURVE')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JNM94XCKCis"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">  \n",
        "<h1><center><strong>Conclusion üìù</strong></center></h1>\n",
        "    <p>\n",
        "<li>We used the twitter sentiment analysis dataset and explored the data with different ways.</li>\n",
        "        <li>We prepared the text data of tweets by removing the unnecessary things.</li>\n",
        "          <li>We trained model based on tensorflow with all settings. </li>\n",
        "        <li>We evaluated thye model with different evaluation measures.</li>\n",
        "         <li>If you are interested to work on any text based project, you can simply apply the same methodolgy but might be you will need to change little settings like name of coloumns etc.</li>\n",
        "        <li>We worked on the classification problem and sepcifically we call it binary classification which is two class classification.</li>\n",
        "        </p>\n",
        "</div>"
      ]
    }
  ]
}