{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign4_Q2_Twitter_114.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gr-30/DL/blob/main/Assign4_Q2_Twitter_114_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPJcfIlrg8Nl",
        "outputId": "654b04fc-5b7f-4e15-da99-ec053c9b2f4f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6p4gdM_Feqb"
      },
      "source": [
        "# 1. Import Libraries/Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSIUM6YNFkEk"
      },
      "source": [
        "## a. Import the required libraries and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E4TvIVLMa73",
        "scrolled": true,
        "trusted": false
      },
      "source": [
        "import os\n",
        "seed=21\n",
        "os.environ['PYTHONHASHSEED']=str(seed)\n",
        "import copy\n",
        "import time\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import rcParams\n",
        "from collections import Counter\n",
        "import tensorflow.keras.regularizers as reg\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def reset_random_seeds(seed_val):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed_val)\n",
        "    tf.random.set_seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "\n",
        "reset_random_seeds(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juSMkpM6F1_Z"
      },
      "source": [
        "## b. Check the GPU available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQBPgeHjF_9D"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAfFbvzZGQcB"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3v6JNFSKzfw"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Mtech DSE BITS/Sem3/DL/Assignment1/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\", header=None)\n",
        " # As the data has no column titles, we will add our own\n",
        "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiMTd9phGfxV"
      },
      "source": [
        "## a. Print at least two movie reviews from each class of the dataset, for a sanity check that labels match the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbdjOYVkINQ5"
      },
      "source": [
        "df_pos = data[data['label'] == 1]\n",
        "df_neg = data[data['label'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwT3CHBsIPil"
      },
      "source": [
        "print('POSITIVE Label data ')\n",
        "df_pos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb959RLJI7al"
      },
      "source": [
        "print('Negative Label data ')\n",
        "df_neg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOYepGcYKCiZ"
      },
      "source": [
        "#### Five top records of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "o27SeogeKCiZ"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTEv12ZUKCia"
      },
      "source": [
        "#### Five last records of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Cj8PV1c-KCia"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgEQYqKKCia"
      },
      "source": [
        "#### Coloumns/features in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "GsDEwSDGKCia"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NWO_gLOJVgE"
      },
      "source": [
        "data['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsgnRLrKCib"
      },
      "source": [
        "#### Length of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "KGFjX1_KKCib"
      },
      "source": [
        "print('lenght of data is', len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ83WJfUKCib"
      },
      "source": [
        "#### Shape of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "7BloB18eKCib"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUJyZyLFKCic"
      },
      "source": [
        "#### Data information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "trusted": false,
        "id": "RzY8eV-YKCic"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDLzeBmoKCic"
      },
      "source": [
        "#### Data types of all coloumns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "qOGeo1NwKCic"
      },
      "source": [
        "data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn3MeiXbKCic"
      },
      "source": [
        "#### Checking Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "2sis1zAMKCid"
      },
      "source": [
        "np.sum(data.isnull().any(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfQdCQMVKCid"
      },
      "source": [
        "#### Rows and columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "c_t0JWayKCid"
      },
      "source": [
        "print('Count of columns in the data is:  ', len(data.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "qCh06swEKCid"
      },
      "source": [
        "print('Count of rows in the data is:  ', len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "7uH3hZt1KCie"
      },
      "source": [
        "data=data[['text','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk_M2ylaKjKv"
      },
      "source": [
        "## b. Plot a bar graph of class distribution in a dataset. Each bar depicts the number of tweets belonging to a particular sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuTDouhfKq5c"
      },
      "source": [
        "datag = data.groupby('label').size().reset_index(name='count')\n",
        "sns.barplot(y=\"count\",x=\"label\", data=datag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzQtvtpAK9S3"
      },
      "source": [
        "## c. Other visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG7Oj0oILAEK"
      },
      "source": [
        "stopwords_ = set(STOPWORDS)\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        collocations=False,\n",
        "        background_color='white',\n",
        "        stopwords=stopwords_,\n",
        "        max_words=200,\n",
        "        max_font_size=40, \n",
        "        scale=3,\n",
        "        random_state=1\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(12, 12))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FILh_fxHLZYO"
      },
      "source": [
        "show_wordcloud(data['text'], \"WordCloud for tweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1LHirJ9L52M"
      },
      "source": [
        "# Data Pre-processing\n",
        "a. Need for this Step-Since the models we use cannot accept string inputs or cannot be of the string format. We have to come up with a way of handling this step. The discussion of different ways of handling this step is out of the scope of this assignment.\n",
        "\n",
        "b. Please use this pre-trained embedding layerfrom TensorFlow hub for this assignment. This link also has a code snippet on how to convert a sentence to a vector. Refer to that for further clarity on this subject.\n",
        "\n",
        "c. Bring the train and test data in the required format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi8vSNbpKCie"
      },
      "source": [
        "## Assigning 1 to Positive sentment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "6ONumEf8KCie"
      },
      "source": [
        "data['label'][data['label']==4]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmNpy1QpKCim"
      },
      "source": [
        "##  Separating input feature and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "b4h-3yaRKCim"
      },
      "source": [
        "X=data.text\n",
        "y=data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50j7wbfOqM3v"
      },
      "source": [
        "### c. Bring the train and test data in the required format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxapX0rxdfaF"
      },
      "source": [
        "#### Separating the 80% data for training data and 20% for testing data\n",
        "As we prepared all the tweets, now we are separating/splitting the tweets into training data and testing data.\n",
        "- 80% tweets will be used in the training \n",
        "- 20% tweets will be used to test the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHKBHBTmdfaG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "yh-985gTdfaH"
      },
      "source": [
        "X_train_org, X_test_org, Y_train, Y_test = train_test_split(data, y, test_size=0.2, random_state=2)\n",
        "X_train_org, X_val_org, Y_train, Y_val = train_test_split(X_train_org, Y_train, test_size=0.2, random_state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEOdcrgOqEFH"
      },
      "source": [
        "### b. Please use this pre-trained embedding layer from TensorFlow hub for this assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlXrUuTPX6We"
      },
      "source": [
        "X_train = embed(X_train_org.text.values).numpy()\n",
        "X_val = embed(X_val_org.text.values).numpy()\n",
        "X_test = embed(X_test_org.text.values).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGnU9IYoj4WJ"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KrgHDaFiyzO"
      },
      "source": [
        "## d. Print the shapes of train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfpN3DridfaJ"
      },
      "source": [
        "print('X_train: ', X_train.shape)\n",
        "print('X_test: ', X_test.shape)\n",
        "print('Y_train: ', Y_train.shape)\n",
        "print('Y_test: ', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmWtpqffkKdD"
      },
      "source": [
        "# Model Building\n",
        "a. Sequential Model layers-Use AT LEAST 3 hidden layers with appropriate input for each. Choose the best number for hidden units and give reasons. : **To prepare near best model we gradually decrease number of neurons in hidden layers by dividing the number of neurons in the previous layers by 2. Hence when we start at 64 neurons and have to prepare 2 more layers then one option would be 32 followed by 16**\n",
        "\n",
        "b. Add L2 regularization to all the layers.\n",
        "\n",
        "c. Add one layer of dropout at the appropriate position and give reasons.d.Choose the appropriate activation function for all the layers.e.Print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "jfAbS-DrdfaL"
      },
      "source": [
        "def model_base(): #Defined tensorflow_based_model function for training tenforflow based model\n",
        "    inputs = Input(name='inputs',shape=(128))\n",
        "    layer = Dropout(0.1, name='dropout')(inputs)\n",
        "    layer = Dense(64,name='layer1', activation='relu',kernel_regularizer=reg.L2(0.0005))(layer)\n",
        "    layer = Dense(32,name='layer2', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(16,name='layer3', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(1,name='output', activation='sigmoid')(layer) \n",
        "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
        "    return model #function returning the value when we call it\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, verbose=1,mode='auto', baseline=None, restore_best_weights=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBo5WAObklEv"
      },
      "source": [
        "## e. Print the model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSLtYUvzkeel"
      },
      "source": [
        "model = model_base() # here we are calling the function of created model\n",
        "print('Model summary')\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaDXPiHTdfaM"
      },
      "source": [
        "# Model Compilation\n",
        "a.Compile the model with the appropriate loss function.\n",
        "\n",
        "b.Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.: **We intially choose 0.001(least possible) but later realised that the training is too slow and could be improved without compromising the accuracy of the model so changed the learning rate to 0.005.**\n",
        "\n",
        "c.Use accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "h9hLp6hSdfaM"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(learning_rate=0.005),metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-2KVd0nmq_y"
      },
      "source": [
        "# Model Training\n",
        "a. Train the model for an appropriate number of epochs. Print the train and validation accuracy and loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "b. Plot the loss and accuracy history graphs for both train and validation set. Print the total time taken for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "no0JaydzdfaN"
      },
      "source": [
        "startTime = time.time()\n",
        "history=model.fit(X_train,Y_train,batch_size=128,epochs=100, validation_data=(X_val, Y_val), callbacks=[callback])# here we are starting the training of model by feeding the training data\n",
        "endTime = time.time()\n",
        "print('Training finished in : {}s'.format(round(endTime - startTime, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNR4m6ncPMHG"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0zt0W1aRXSE"
      },
      "source": [
        "print(history.history)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y34PUZ6GnG7b"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "a.Print the final train and validation loss and accuracy. Print confusion matrix and classification report for the validation dataset. Analyse and report the best and worst performing class.\n",
        "\n",
        "b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Hrfd48jTdfaQ"
      },
      "source": [
        "accr1 = model.evaluate(X_val,Y_val) #we are starting to test the model here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "1K3z4Tr9dfaQ"
      },
      "source": [
        "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edTXFrbdfaR"
      },
      "source": [
        "y_pred = model.predict(X_val) #getting predictions on the trained model\n",
        "y_pred_class = (y_pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "bYk61TZldfaR"
      },
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(X_val, y_pred))\n",
        "print(\"Confusion matrix\")\n",
        "CR=confusion_matrix(Y_val, y_pred_class)\n",
        "print(CR)\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMn5BviYnG7e"
      },
      "source": [
        "### a.Negative class or Negative Sentiment is slightly better performing as per the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYNjpGnVdfaS"
      },
      "source": [
        "#### ROC CURVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRV9OfnbM73b"
      },
      "source": [
        "print(roc_auc_score(Y_val, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "9u-PLELfdfaT"
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(Y_val, y_pred_class)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC CURVE')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRXQ1T5Wn3AG"
      },
      "source": [
        "### b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OjQdeMPVhSB"
      },
      "source": [
        "def find_2_smallest_index(arr):\n",
        "    first = 999\n",
        "    second = 999\n",
        "    first_index=0\n",
        "    sexond_index=0\n",
        "    for i in range(len(arr)):\n",
        "        if arr[i] < first:\n",
        "            second = copy.deepcopy(first)\n",
        "            second_index = copy.deepcopy(first_index)\n",
        "            first = copy.deepcopy(arr[i])\n",
        "            first_index = copy.deepcopy(i)\n",
        "        elif (arr[i] < second):\n",
        "            second = copy.deepcopy(arr[i])\n",
        "            second_index = copy.deepcopy(i)\n",
        "    return first_index, second_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKRK7UIPXCm7"
      },
      "source": [
        "worst_2 = {}\n",
        "y_pred_test = model.predict(X_test)\n",
        "first_index, second_index = find_2_smallest_index(1-y_pred_test[:])\n",
        "worst_2[0] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "first_index, second_index = find_2_smallest_index(y_pred_test[:])\n",
        "worst_2[1] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "worst_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CumEnWCvTgHY"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "Build two more additional models by changing the following hyperparameters ONE at a time. Write the code for Model Building, Model Compilation, Model Training and Model Evaluation as given in the instructions above for each additional model.\n",
        "1. Batch Size: Change the value of batch size in model training\n",
        "2. Dropout: Change the position and value of dropout layer\n",
        "Write a comparison between each model and give reasons for the difference in results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvNttYFcywU7"
      },
      "source": [
        "### **1.Batch Size: Change the value of batch size in model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9bHvDx6y-Qv"
      },
      "source": [
        "# Model Building\n",
        "a. Sequential Model layers-Use AT LEAST 3 hidden layers with appropriate input for each. Choose the best number for hidden units and give reasons. : **To prepare near best model we gradually decrease number of neurons in hidden layers by dividing the number of neurons in the previous layers by 2. Hence when we start at 64 neurons and have to prepare 3 more layers then one option would be 32 followed by 16 followed by 8**\n",
        "\n",
        "b. Add L2 regularization to all the layers.\n",
        "\n",
        "c. Add one layer of dropout at the appropriate position and give reasons.d.Choose the appropriate activation function for all the layers.e.Print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "s1VVq0NJTgHV"
      },
      "source": [
        "def model_hyp1(): #Defined tensorflow_based_model function for training tensorflow based model\n",
        "    inputs = Input(name='inputs',shape=(128))\n",
        "    layer = Dropout(0.1, name='dropout')(inputs)\n",
        "    layer = Dense(64,name='layer1', activation='relu',kernel_regularizer=reg.L2(0.0005))(layer)\n",
        "    layer = Dense(32,name='layer2', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(16,name='layer3', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(8,name='layer4', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer)\n",
        "    layer = Dense(1,name='output', activation='sigmoid')(layer) \n",
        "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
        "    return model #function returning the value when we call it\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, verbose=1,mode='auto', baseline=None, restore_best_weights=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrQlXs8Ez3FJ"
      },
      "source": [
        "## e. Print the model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "l8OhDwLVTgHZ"
      },
      "source": [
        "model = model_hyp1() # here we are calling the function of created model\n",
        "print('Model summary')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqMWN4vV0vpb"
      },
      "source": [
        "# Model Compilation\n",
        "a.Compile the model with the appropriate loss function.\n",
        "\n",
        "b.Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.: **We intially choose 0.001(least possible) but later realised that the training is too slow and could be improved without compromising the accuracy of the model so changed the learning rate to 0.005.**\n",
        "\n",
        "c.Use accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6CblQB40qGf"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(learning_rate=0.005),metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tby5ys3c04mg"
      },
      "source": [
        "# Model Training\n",
        "a. Train the model for an appropriate number of epochs. Print the train and validation accuracy and loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "b. Plot the loss and accuracy history graphs for both train and validation set. Print the total time taken for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "0wftljwQTgHc"
      },
      "source": [
        "startTime = time.time()\n",
        "history=model.fit(X_train,Y_train,batch_size=128,epochs=100, validation_split=0.2, callbacks=[callback])# here we are starting the training of model by feeding the training data\n",
        "endTime = time.time()\n",
        "print('Training finished in : {}s'.format(round(endTime - startTime, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzCvFyWdTgHe"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0TXt1y_TgHg"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ERF1DUU9BI_"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "a.Print the final train and validation loss and accuracy. Print confusion matrix and classification report for the validation dataset. Analyse and report the best and worst performing class.\n",
        "\n",
        "b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "nzZOkOOe9BJD"
      },
      "source": [
        "accr1 = model.evaluate(X_val,Y_val) #we are starting to test the model here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "v9GaHB_K9BJF"
      },
      "source": [
        "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BybSBr7h9BJG"
      },
      "source": [
        "y_pred = model.predict(X_val) #getting predictions on the trained model\n",
        "y_pred_class = (y_pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "3Dxj_ycd9BJG"
      },
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(X_val, y_pred))\n",
        "print(\"Confusion matrix\")\n",
        "CR=confusion_matrix(Y_val, y_pred_class)\n",
        "print(CR)\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5kLPOB59BJH"
      },
      "source": [
        "### a.Negative class or Negative Sentiment is slightly better performing as per the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft7avBSQ9BJH"
      },
      "source": [
        "#### ROC CURVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPQ1F3Qk9BJI"
      },
      "source": [
        "print(roc_auc_score(Y_val, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "hJtIlaG69BJI"
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(Y_val, y_pred_class)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC CURVE')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGF2WSYL9BJJ"
      },
      "source": [
        "### b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbTZWAsZ9BJJ"
      },
      "source": [
        "def find_2_smallest_index(arr):\n",
        "    first = 999\n",
        "    second = 999\n",
        "    first_index=0\n",
        "    sexond_index=0\n",
        "    for i in range(len(arr)):\n",
        "        if arr[i] < first:\n",
        "            second = copy.deepcopy(first)\n",
        "            second_index = copy.deepcopy(first_index)\n",
        "            first = copy.deepcopy(arr[i])\n",
        "            first_index = copy.deepcopy(i)\n",
        "        elif (arr[i] < second):\n",
        "            second = copy.deepcopy(arr[i])\n",
        "            second_index = copy.deepcopy(i)\n",
        "    return first_index, second_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv4cHxvj9BJJ"
      },
      "source": [
        "worst_2 = {}\n",
        "y_pred_test = model.predict(X_test)\n",
        "first_index, second_index = find_2_smallest_index(1-y_pred_test[:])\n",
        "worst_2[0] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "first_index, second_index = find_2_smallest_index(y_pred_test[:])\n",
        "worst_2[1] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "worst_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viGLHfOvThk-"
      },
      "source": [
        "### **2.Optimiser: Use a different optimizer with the appropriate LR value**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB1tByMy4kw1"
      },
      "source": [
        "# Model Building\n",
        "a. Sequential Model layers-Use AT LEAST 3 hidden layers with appropriate input for each. Choose the best number for hidden units and give reasons. : **To prepare near best model we gradually decrease number of neurons in hidden layers by dividing the number of neurons in the previous layers by 2. Hence when we start at 64 neurons and have to prepare 2 more layers then one option would be 32 followed by 16**\n",
        "\n",
        "b. Add L2 regularization to all the layers.\n",
        "\n",
        "c. Add one layer of dropout at the appropriate position and give reasons.d.Choose the appropriate activation function for all the layers.e.Print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "bm_GZDtdThk9"
      },
      "source": [
        "def model_hyp2(): #Defined tensorflow_based_model function for training tenforflow based model\n",
        "    inputs = Input(name='inputs',shape=(128))\n",
        "    layer = Dropout(0.1, name='dropout')(inputs)\n",
        "    layer = Dense(64,name='layer1', activation='relu',kernel_regularizer=reg.L2(0.0005))(inputs)\n",
        "    layer = Dense(32,name='layer2', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(16,name='layer3', activation='relu', kernel_regularizer=reg.L2(0.0005))(layer) \n",
        "    layer = Dense(1,name='output', activation='sigmoid')(layer) \n",
        "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
        "    return model #function returning the value when we call it\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, verbose=1,mode='auto', baseline=None, restore_best_weights=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Qb48Q74yL2"
      },
      "source": [
        "## e. Print the model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "1eWzMJ3m4yL9"
      },
      "source": [
        "model = model_hyp1() # here we are calling the function of created model\n",
        "print('Model summary')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyhJ6hvb446g"
      },
      "source": [
        "# Model Compilation\n",
        "a.Compile the model with the appropriate loss function.\n",
        "\n",
        "b.Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.: **We chose adam optimizer to incorporate the momentum component in the optimization process. We intially choose 0.001(least possible) but later realised that the training is too slow and could be improved without compromising the accuracy of the model so changed the learning rate to 0.005.**\n",
        "\n",
        "c.Use accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "w4YCDA1nThk_"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.005),metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkAyHHt5PDP"
      },
      "source": [
        "# Model Training\n",
        "a. Train the model for an appropriate number of epochs. Print the train and validation accuracy and loss for each epoch. Use the appropriate batch size.\n",
        "\n",
        "b. Plot the loss and accuracy history graphs for both train and validation set. Print the total time taken for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "bQpeSia1ThlA"
      },
      "source": [
        "startTime = time.time()\n",
        "history=model.fit(X_train,Y_train,batch_size=128,epochs=100, validation_split=0.2, callbacks=[callback])# here we are starting the training of model by feeding the training data\n",
        "endTime = time.time()\n",
        "print('Training finished in : {}s'.format(round(endTime - startTime, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODAs5ViWThlB"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8dASWrfThlC"
      },
      "source": [
        "print(history.history)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB8jxzLE9H5B"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "a.Print the final train and validation loss and accuracy. Print confusion matrix and classification report for the validation dataset. Analyse and report the best and worst performing class.\n",
        "\n",
        "b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "DqQZkk4B9H5C"
      },
      "source": [
        "accr1 = model.evaluate(X_val,Y_val) #we are starting to test the model here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "XY9KHLlU9H5C"
      },
      "source": [
        "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYlnpCyF9H5C"
      },
      "source": [
        "y_pred = model.predict(X_val) #getting predictions on the trained model\n",
        "y_pred_class = (y_pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "27e29WtT9H5C"
      },
      "source": [
        "print('Classification Report')\n",
        "print(classification_report(X_val, y_pred))\n",
        "print(\"Confusion matrix\")\n",
        "CR=confusion_matrix(Y_val, y_pred_class)\n",
        "print(CR)\n",
        "fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAashWtb9H5D"
      },
      "source": [
        "### a.Negative class or Negative Sentiment is slightly better performing as per the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57QyxSsz9H5D"
      },
      "source": [
        "#### ROC CURVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXKyRmmE9H5D"
      },
      "source": [
        "print(roc_auc_score(Y_val, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "p9qmngjy9H5D"
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(Y_val, y_pred_class)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC CURVE')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj5kaUiN9H5D"
      },
      "source": [
        "### b.Print the two most incorrectly classified texts for each class in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxM1IHpf9H5D"
      },
      "source": [
        "def find_2_smallest_index(arr):\n",
        "    first = 999\n",
        "    second = 999\n",
        "    first_index=0\n",
        "    sexond_index=0\n",
        "    for i in range(len(arr)):\n",
        "        if arr[i] < first:\n",
        "            second = copy.deepcopy(first)\n",
        "            second_index = copy.deepcopy(first_index)\n",
        "            first = copy.deepcopy(arr[i])\n",
        "            first_index = copy.deepcopy(i)\n",
        "        elif (arr[i] < second):\n",
        "            second = copy.deepcopy(arr[i])\n",
        "            second_index = copy.deepcopy(i)\n",
        "    return first_index, second_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M04c8n99H5E"
      },
      "source": [
        "worst_2 = {}\n",
        "y_pred_test = model.predict(X_test)\n",
        "first_index, second_index = find_2_smallest_index(1-y_pred_test[:])\n",
        "worst_2[0] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "first_index, second_index = find_2_smallest_index(y_pred_test[:])\n",
        "worst_2[1] = [X_test_org.text.values[first_index],X_test_org.text.values[second_index]]\n",
        "worst_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mMJPf5958D5"
      },
      "source": [
        "# Write a comparison between each model and give reasons for the difference in results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SQ9ofkK6AH5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}